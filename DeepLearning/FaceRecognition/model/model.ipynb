{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> Deep Face Detection Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install matplotlib albumentations --q\n",
    "%pip install tensorflow-macos --q\n",
    "%pip install labelme --q\n",
    "\n",
    "#List of Dependencies that are installed\n",
    "%pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "import os\n",
    "import time\n",
    "import uuid #Unique File Identifiers for the Images\n",
    "import cv2\n",
    "import json \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capture Images used for Data Segement and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Paths \n",
    "image_path = os.path.join('/Applications/Deep Learning/Face Detection/data', 'images')\n",
    "number_of_images = 30\n",
    "\n",
    "#If Directory Does not Exist\n",
    "os.makedirs(image_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open video source \n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    answer = input(\"Do you wish to capture Images(Y/N): \").lower()\n",
    "    if answer != 'y':\n",
    "        break\n",
    "    for image_number in range(number_of_images):\n",
    "        print('Collecting Image {}'.format(image_number))\n",
    "        return_val, frame = cap.read() #Frame Number and Boolean Val for Success\n",
    "\n",
    "        if not return_val:\n",
    "            print('Image did not Successfully Capture')\n",
    "\n",
    "        else:\n",
    "            print('Picture Captured Successfully')\n",
    "        img_name = os.path.join(image_path, f'{str(uuid.uuid1())}.jpg') #Unique UUID per image\n",
    "        cv2.imwrite(img_name, frame) #Save Image\n",
    "        cv2.imshow('frame', frame) #Display Image in Frame\n",
    "        time.sleep(0.5)\n",
    "\n",
    "        #Break out of Capturing Images if error occurs\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "   \n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate DataSet and Build Image Loading Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: labelme\n"
     ]
    }
   ],
   "source": [
    "#Create a Virtual Enviornment Just for LabelMe\n",
    "''' \n",
    "conda create -n labelme python=3.10\n",
    "conda activate labelme\n",
    "pip install labelme\n",
    "labelme\n",
    "conda activate labelme (Use to Label the Images)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/00/s1_j28lx7ms0m5jvxr8zly800000gn/T/ipykernel_1445/1509499387.py:2: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "GPU Avaliable: False\n"
     ]
    }
   ],
   "source": [
    "#GPU Avaliable to be used?\n",
    "print('GPU Avaliable:', tf.test.is_gpu_available('GPU'))\n",
    "\n",
    "''' \n",
    "Load Images into TF Data Pipeline\n",
    "Create Tensorflow of datasets, shuffle enables not inherent order)\n",
    "'''\n",
    "images = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/data/images/*.jpg', shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Reads entire filestream of images and decodes JPEG file into 3D Tensor\n",
    "Converts Images into Useable Tensor Datatype\n",
    "'''\n",
    "def load_image(img):\n",
    "    byte_img = tf.io.read_file(img)\n",
    "    image = tf.io.decode_jpeg(byte_img)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map function that applies function to each item\n",
    "images = images.map(load_image)\n",
    "\n",
    "#Iterate through the images\n",
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Raw Images in Matplotlib\n",
    "image_generator = images.batch(10).as_numpy_iterator()\n",
    "print(type(image_generator))\n",
    "next_batch = next(image_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.next() contains the index and the image using .as_numpy_iterator()\n",
    "while True:\n",
    "    answer = input('Next Batch: ').lower()\n",
    "    if answer != 'y':\n",
    "        break\n",
    "    else:\n",
    "        plt.clf()\n",
    "        next_batch = next(image_generator)\n",
    "    \n",
    "        fig, ax = plt.subplots(ncols=5, nrows=2, figsize=(10,10))\n",
    "        for idx, image in enumerate(next_batch):\n",
    "            row = idx//5\n",
    "            col = idx%5\n",
    "            ax[row,col].imshow(image)\n",
    "            ax[row,col].axis('off')\n",
    "        plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition Unaugmented Data\n",
    "\n",
    "* Note: It is possible to do the following task using split_folders (more standard in DL ). Higher level command and ensures that on any OS that the command works correctly. In addition splitting between the Data and the corresponding Label Class is also automatic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Images in Dataset: 382\n",
      "Train:  267\n",
      "Validation: 57\n",
      "Test: 58\n"
     ]
    }
   ],
   "source": [
    "#Total Images is 382\n",
    "print('Total Images in Dataset:', len([f for f in os.listdir(image_path)]))\n",
    "\n",
    "\n",
    "total_images = [f for f in os.listdir(image_path)]\n",
    "\n",
    "#Train Dataset should be 70% of dataset\n",
    "train_set, temp_set = train_test_split(total_images, test_size=0.3, random_state = 42)\n",
    "\n",
    "#Validation and Test will be 15% each\n",
    "validation_set, test_set = train_test_split(temp_set, test_size=0.5, random_state = 42)\n",
    "\n",
    "print('Train: ', len(train_set))\n",
    "print(\"Validation:\", len(validation_set))\n",
    "print(\"Test:\", len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Applications/Deep Learning/Face Detection/data/images'\n",
    "base_path = '/Applications/Deep Learning/Face Detection/data'\n",
    "\n",
    "\n",
    "#Put each Train Test Set into Respective Folder\n",
    "train_dataset = os.path.join(base_path, 'train_set/images')\n",
    "validation_dataset = os.path.join(base_path, 'validation_set/images')\n",
    "test_dataset = os.path.join(base_path, 'test_set/images')\n",
    "\n",
    "\n",
    "#Put Images in Respective Folder\n",
    "for img in train_set:\n",
    "     shutil.copy(os.path.join(image_path, img), os.path.join(train_dataset, img))\n",
    "for img in validation_set:\n",
    "     shutil.copy(os.path.join(image_path, img), os.path.join(validation_dataset, img))\n",
    "\n",
    "for img in test_set:\n",
    "     shutil.copy(os.path.join(image_path, img), os.path.join(test_dataset, img))\n",
    "\n",
    "#Create Tensorflow dataset (Images)\n",
    "train_dataset = tf.data.Dataset.list_files(os.path.join(train_dataset, '*.jpg'))\n",
    "validation_dataset   = tf.data.Dataset.list_files(os.path.join(validation_dataset, '*.jpg'))\n",
    "test_dataset  = tf.data.Dataset.list_files(os.path.join(test_dataset, '*.jpg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30dfb86a-eb17-11f0-93ba-c64476ca29a9\n",
      "508bdf18-eb17-11f0-93ba-c64476ca29a9\n",
      "624a4632-eb16-11f0-93ba-c64476ca29a9\n",
      "63f82fb2-eb16-11f0-93ba-c64476ca29a9\n",
      "644c3710-eb16-11f0-93ba-c64476ca29a9\n",
      "64f965e8-eb16-11f0-93ba-c64476ca29a9\n",
      "654f8d1a-eb16-11f0-93ba-c64476ca29a9\n",
      "65a5b190-eb16-11f0-93ba-c64476ca29a9\n",
      "6e9512cc-eb17-11f0-93ba-c64476ca29a9\n",
      "8dc75d2a-eb18-11f0-93ba-c64476ca29a9\n",
      "8ecb3f16-eb18-11f0-93ba-c64476ca29a9\n",
      "8f23390a-eb18-11f0-93ba-c64476ca29a9\n",
      "8f61676c-eb17-11f0-93ba-c64476ca29a9\n",
      "91bd5a8e-eb17-11f0-93ba-c64476ca29a9\n",
      "91d5d1c6-eb18-11f0-93ba-c64476ca29a9\n",
      "922c3dfe-eb18-11f0-93ba-c64476ca29a9\n",
      "93dd96ac-eb18-11f0-93ba-c64476ca29a9\n",
      "b19c3e2e-eb17-11f0-93ba-c64476\n"
     ]
    }
   ],
   "source": [
    "#This is not an efficient solution but it does the trick in aligning corresponding label to img\n",
    "label_path = '/Applications/Deep Learning/Face Detection/data/labels'\n",
    "train_labels = [\"00717294-eb18-11f0-93ba-c64476ca29a9\", \"00c7b1fe-eb18-11f0-93ba-c64476ca29a9\", \"011e76f6-eb18-11f0-93ba-c64476ca29a9\", \"0174d6f4-eb18-11f0-93ba-c64476ca29a9\", \"2cd419dc-eb17-11f0-93ba-c64476ca29a9\", \"2d287324-eb17-11f0-93ba-c64476ca29a9\", \"2d8081fe-eb17-11f0-93ba-c64476ca29a9\", \"2e2bf7b4-eb17-11f0-93ba-c64476ca29a9\", \"2edaaf20-eb17-11f0-93ba-c64476ca29a9\", \"2f2efe9a-eb17-11f0-93ba-c64476ca29a9\", \"2f873c04-eb17-11f0-93ba-c64476ca29a9\", \"30328532-eb17-11f0-93ba-c64476ca29a9\", \"308945fc-eb17-11f0-93ba-c64476ca29a9\", \"30dfb86a-eb17-11f0-93ba-c64476ca29a9\", \"3137cfdc-eb17-11f0-93ba-c64476ca29a9\", \"318ea8ca-eb17-11f0-93ba-c64476ca29a9\", \"31e4e1e0-eb17-11f0-93ba-c64476ca29a9\", \"323b67ea-eb17-11f0-93ba-c64476ca29a9\", \"32e71c20-eb17-11f0-93ba-c64476ca29a9\", \"333d6c2e-eb17-11f0-93ba-c64476ca29a9\", \"33954c00-eb17-11f0-93ba-c64476ca29a9\", \"33ea91f6-eb17-11f0-93ba-c64476ca29a9\", \"34978708-eb17-11f0-93ba-c64476ca29a9\", \"34ee40fc-eb17-11f0-93ba-c64476ca29a9\", \"35f19170-eb17-11f0-93ba-c64476ca29a9\", \"3648aef6-eb17-11f0-93ba-c64476ca29a9\", \"369e2034-eb17-11f0-93ba-c64476ca29a9\", \"4717bb46-eb17-11f0-93ba-c64476ca29a9\", \"476b60c0-eb17-11f0-93ba-c64476ca29a9\", \"47c24552-eb17-11f0-93ba-c64476ca29a9\", \"49c8fdd2-eb17-11f0-93ba-c64476ca29a9\", \"4a202062-eb17-11f0-93ba-c64476ca29a9\", \"4a761d96-eb17-11f0-93ba-c64476ca29a9\", \"4b22aca0-eb17-11f0-93ba-c64476ca29a9\", \"4b7a4758-eb17-11f0-93ba-c64476ca29a9\", \"4bd011d8-eb17-11f0-93ba-c64476ca29a9\", \"4c273120-eb17-11f0-93ba-c64476ca29a9\", \"4c7d470e-eb17-11f0-93ba-c64476ca29a9\", \"4cd3f0cc-eb17-11f0-93ba-c64476ca29a9\", \"4d2a28e8-eb17-11f0-93ba-c64476ca29a9\", \"4d810708-eb17-11f0-93ba-c64476ca29a9\", \"4dd77caa-eb17-11f0-93ba-c64476ca29a9\", \"4e842216-eb17-11f0-93ba-c64476ca29a9\", \"4edb56ee-eb17-11f0-93ba-c64476ca29a9\", \"4f308a10-eb17-11f0-93ba-c64476ca29a9\", \"4f88b7b2-eb17-11f0-93ba-c64476ca29a9\", \"4fde6a18-eb17-11f0-93ba-c64476ca29a9\", \"5035b138-eb17-11f0-93ba-c64476ca29a9\", \"508bdf18-eb17-11f0-93ba-c64476ca29a9\", \"5fd29d32-eb16-11f0-93ba-c64476ca29a9\", \"608dbee6-eb16-11f0-93ba-c64476ca29a9\", \"60e406ca-eb16-11f0-93ba-c64476ca29a9\", \"619bf3b6-eb16-11f0-93ba-c64476ca29a9\", \"624a4632-eb16-11f0-93ba-c64476ca29a9\", \"62a05c34-eb16-11f0-93ba-c64476ca29a9\", \"62f67a06-eb16-11f0-93ba-c64476ca29a9\", \"63f82fb2-eb16-11f0-93ba-c64476ca29a9\", \"644c3710-eb16-11f0-93ba-c64476ca29a9\", \"64a24380-eb16-11f0-93ba-c64476ca29a9\", \"64f965e8-eb16-11f0-93ba-c64476ca29a9\", \"654f8d1a-eb16-11f0-93ba-c64476ca29a9\", \"65a5b190-eb16-11f0-93ba-c64476ca29a9\", \"65fc70de-eb16-11f0-93ba-c64476ca29a9\", \"6b302374-eb17-11f0-93ba-c64476ca29a9\", \"6b898702-eb17-11f0-93ba-c64476ca29a9\", \"6be282d0-eb17-11f0-93ba-c64476ca29a9\", \"6c36b13e-eb17-11f0-93ba-c64476ca29a9\", \"6ce3be38-eb17-11f0-93ba-c64476ca29a9\", \"6d3a59aa-eb17-11f0-93ba-c64476ca29a9\", \"6de84538-eb17-11f0-93ba-c64476ca29a9\", \"6e3e6c74-eb17-11f0-93ba-c64476ca29a9\", \"6e9512cc-eb17-11f0-93ba-c64476ca29a9\", \"6eeb308a-eb17-11f0-93ba-c64476ca29a9\", \"6f97e2c6-eb17-11f0-93ba-c64476ca29a9\", \"709a6dce-eb17-11f0-93ba-c64476ca29a9\", \"70f242a6-eb17-11f0-93ba-c64476ca29a9\", \"71476d80-eb17-11f0-93ba-c64476ca29a9\", \"719f025c-eb17-11f0-93ba-c64476ca29a9\", \"724b4a26-eb17-11f0-93ba-c64476ca29a9\", \"72f941da-eb17-11f0-93ba-c64476ca29a9\", \"7350227a-eb17-11f0-93ba-c64476ca29a9\", \"73a63f34-eb17-11f0-93ba-c64476ca29a9\", \"74529838-eb17-11f0-93ba-c64476ca29a9\", \"74a9fde4-eb17-11f0-93ba-c64476ca29a9\", \"7500695e-eb17-11f0-93ba-c64476ca29a9\", \"779b3856-eb17-11f0-93ba-c64476ca29a9\", \"77f0387e-eb17-11f0-93ba-c64476ca29a9\", \"7846f5ec-eb17-11f0-93ba-c64476ca29a9\", \"794b745e-eb17-11f0-93ba-c64476ca29a9\", \"7a4f1b58-eb17-11f0-93ba-c64476ca29a9\", \"7aa41478-eb17-11f0-93ba-c64476ca29a9\", \"7afc04d0-eb17-11f0-93ba-c64476ca29a9\", \"7bfef96e-eb17-11f0-93ba-c64476ca29a9\", \"7c56c40a-eb17-11f0-93ba-c64476ca29a9\", \"7cad190e-eb17-11f0-93ba-c64476ca29a9\", \"7d58664c-eb17-11f0-93ba-c64476ca29a9\", \"7db0d5b6-eb17-11f0-93ba-c64476ca29a9\", \"7e06aee6-eb17-11f0-93ba-c64476ca29a9\", \"7eb29d0a-eb17-11f0-93ba-c64476ca29a9\", \"7f60b368-eb17-11f0-93ba-c64476ca29a9\", \"7fb639f0-eb17-11f0-93ba-c64476ca29a9\", \"805e0aa4-eb17-11f0-93ba-c64476ca29a9\", \"80b59fa8-eb17-11f0-93ba-c64476ca29a9\", \"810accf8-eb17-11f0-93ba-c64476ca29a9\", \"8162a950-eb17-11f0-93ba-c64476ca29a9\", \"8a49e3fc-eb18-11f0-93ba-c64476ca29a9\", \"8b14ccfc-eb18-11f0-93ba-c64476ca29a9\", \"8b6b28b8-eb18-11f0-93ba-c64476ca29a9\", \"8c171c90-eb18-11f0-93ba-c64476ca29a9\", \"8c6d5b3c-eb18-11f0-93ba-c64476ca29a9\", \"8cb7a616-eb17-11f0-93ba-c64476ca29a9\", \"8cc568c2-eb18-11f0-93ba-c64476ca29a9\", \"8d0d89f0-eb17-11f0-93ba-c64476ca29a9\", \"8d1b2ce4-eb18-11f0-93ba-c64476ca29a9\", \"8d65289a-eb17-11f0-93ba-c64476ca29a9\", \"8d70f368-eb18-11f0-93ba-c64476ca29a9\", \"8dbb3546-eb17-11f0-93ba-c64476ca29a9\", \"8dc75d2a-eb18-11f0-93ba-c64476ca29a9\", \"8e1e90c2-eb18-11f0-93ba-c64476ca29a9\", \"8e62f06a-eb17-11f0-93ba-c64476ca29a9\", \"8e74ada4-eb18-11f0-93ba-c64476ca29a9\", \"8eb8eef2-eb17-11f0-93ba-c64476ca29a9\", \"8ecb3f16-eb18-11f0-93ba-c64476ca29a9\", \"8f0e7f84-eb17-11f0-93ba-c64476ca29a9\", \"8f23390a-eb18-11f0-93ba-c64476ca29a9\", \"8f61676c-eb17-11f0-93ba-c64476ca29a9\", \"8f798170-eb18-11f0-93ba-c64476ca29a9\", \"8fcf5da2-eb18-11f0-93ba-c64476ca29a9\", \"900d8006-eb17-11f0-93ba-c64476ca29a9\", \"9026950e-eb18-11f0-93ba-c64476ca29a9\", \"90d3af28-eb18-11f0-93ba-c64476ca29a9\", \"9110e2f4-eb17-11f0-93ba-c64476ca29a9\", \"912a9810-eb18-11f0-93ba-c64476ca29a9\", \"9167dac8-eb17-11f0-93ba-c64476ca29a9\", \"91805d04-eb18-11f0-93ba-c64476ca29a9\", \"91bd5a8e-eb17-11f0-93ba-c64476ca29a9\", \"91d5d1c6-eb18-11f0-93ba-c64476ca29a9\", \"9215c76e-eb17-11f0-93ba-c64476ca29a9\", \"922c3dfe-eb18-11f0-93ba-c64476ca29a9\", \"9282db82-eb18-11f0-93ba-c64476ca29a9\", \"92c219e2-eb17-11f0-93ba-c64476ca29a9\", \"92d91bbe-eb18-11f0-93ba-c64476ca29a9\", \"93187d78-eb17-11f0-93ba-c64476ca29a9\", \"933177d2-eb18-11f0-93ba-c64476ca29a9\", \"93703b6c-eb17-11f0-93ba-c64476ca29a9\", \"9387f864-eb18-11f0-93ba-c64476ca29a9\", \"93c5b786-eb17-11f0-93ba-c64476ca29a9\", \"93dd96ac-eb18-11f0-93ba-c64476ca29a9\", \"941ccdbe-eb17-11f0-93ba-c64476ca29a9\", \"9434df98-eb18-11f0-93ba-c64476ca29a9\", \"94730378-eb17-11f0-93ba-c64476ca29a9\", \"95cd2b86-eb17-11f0-93ba-c64476ca29a9\", \"9623f6d2-eb17-11f0-93ba-c64476ca29a9\", \"9679d854-eb17-11f0-93ba-c64476ca29a9\", \"a23e1088-eb17-11f0-93ba-c64476ca29a9\", \"a2937f1e-eb17-11f0-93ba-c64476ca29a9\", \"a2e6a9d2-eb17-11f0-93ba-c64476ca29a9\", \"a33ba8c4-eb17-11f0-93ba-c64476ca29a9\", \"a39416ee-eb17-11f0-93ba-c64476ca29a9\", \"a3e8716c-eb17-11f0-93ba-c64476ca29a9\", \"a440c150-eb17-11f0-93ba-c64476ca29a9\", \"a4958988-eb17-11f0-93ba-c64476ca29a9\", \"a4ec86ac-eb17-11f0-93ba-c64476ca29a9\", \"a599c0b0-eb17-11f0-93ba-c64476ca29a9\", \"a5efc1ea-eb17-11f0-93ba-c64476ca29a9\", \"ace1b21a-eb17-11f0-93ba-c64476ca29a9\", \"ad8e5da8-eb17-11f0-93ba-c64476ca29a9\", \"ae3b5dc8-eb17-11f0-93ba-c64476ca29a9\", \"ae91c398-eb17-11f0-93ba-c64476ca29a9\", \"af3eece4-eb17-11f0-93ba-c64476ca29a9\", \"af953bf8-eb17-11f0-93ba-c64476ca29a9\", \"afeb7126-eb17-11f0-93ba-c64476ca29a9\", \"b042b698-eb17-11f0-93ba-c64476ca29a9\", \"b0981480-eb17-11f0-93ba-c64476ca29a9\", \"b0eee292-eb17-11f0-93ba-c64476ca29a9\", \"b145210c-eb17-11f0-93ba-c64476ca29a9\", \"b19c3e2e-eb17-11f0-93ba-c64476\"]\n",
    "test_labels = [\"2dd6eca6-eb17-11f0-93ba-c64476ca29a9\",\"2e8357de-eb17-11f0-93ba-c64476ca29a9\",\"34410248-eb17-11f0-93ba-c64476ca29a9\",\"354559c8-eb17-11f0-93ba-c64476ca29a9\",\"48176e88-eb17-11f0-93ba-c64476ca29a9\",\"4e2e0b56-eb17-11f0-93ba-c64476ca29a9\",\"50e20938-eb17-11f0-93ba-c64476ca29a9\",\"61f2bb7e-eb16-11f0-93ba-c64476ca29a9\",\"634dbfa0-eb16-11f0-93ba-c64476ca29a9\",\"6c8d3ce8-eb17-11f0-93ba-c64476ca29a9\",\"6f421292-eb17-11f0-93ba-c64476ca29a9\",\"7045498e-eb17-11f0-93ba-c64476ca29a9\",\"72a24ec0-eb17-11f0-93ba-c64476ca29a9\",\"78f5070e-eb17-11f0-93ba-c64476ca29a9\",\"79f8417a-eb17-11f0-93ba-c64476ca29a9\",\"7b5246a6-eb17-11f0-93ba-c64476ca29a9\",\"7d03b6c4-eb17-11f0-93ba-c64476ca29a9\",\"7f0926b6-eb17-11f0-93ba-c64476ca29a9\",\"8ab2ceb2-eb18-11f0-93ba-c64476ca29a9\",\"8fb7f726-eb17-11f0-93ba-c64476ca29a9\",\"90ba15fa-eb17-11f0-93ba-c64476ca29a9\",\"9576deac-eb17-11f0-93ba-c64476ca29a9\",\"a1e8c57e-eb17-11f0-93ba-c64476ca29a9\",\"a542fe60-eb17-11f0-93ba-c64476ca29a9\",\"a6463c64-eb17-11f0-93ba-c64476ca29a9\",\"ade4864c-eb17-11f0-93ba-c64476ca29a9\",\"b2f6a41c-eb17-11f0-93ba-c64476ca29a9\",\"b4fdd2d0-eb17-11f0-93ba-c64476ca29a9\",\"ba1445ec-eb17-11f0-93ba-c64476ca29a9\",\"bb165340-eb17-11f0-93ba-c64476ca29a9\",\"bb6e9df2-eb17-11f0-93ba-c64476ca29a9\",\"bc7055f6-eb17-11f0-93ba-c64476ca29a9\",\"bd1e3842-eb17-11f0-93ba-c64476ca29a9\",\"bd74318e-eb17-11f0-93ba-c64476ca29a9\",\"dbba2a96-eb16-11f0-93ba-c64476ca29a9\",\"de427f7a-eb16-11f0-93ba-c64476ca29a9\",\"de9544da-eb16-11f0-93ba-c64476ca29a9\",\"dfef42fe-eb16-11f0-93ba-c64476ca29a9\",\"e3a58dc2-eb16-11f0-93ba-c64476ca29a9\",\"e6095f62-eb16-11f0-93ba-c64476ca29a9\",\"e6b56c08-eb16-11f0-93ba-c64476ca29a9\",\"e7b8c6f4-eb16-11f0-93ba-c64476ca29a9\",\"eabef95e-eb16-11f0-93ba-c64476ca29a9\",\"ec198486-eb16-11f0-93ba-c64476ca29a9\",\"ed1cd7c0-eb16-11f0-93ba-c64476ca29a9\",\"ed72cfe0-eb16-11f0-93ba-c64476ca29a9\",\"edc8bda6-eb16-11f0-93ba-c64476ca29a9\",\"f17b7bdc-eb16-11f0-93ba-c64476ca29a9\",\"f1e0ad08-eb17-11f0-93ba-c64476ca29a9\",\"f28d7b14-eb17-11f0-93ba-c64476ca29a9\",\"f4eb0854-eb17-11f0-93ba-c64476ca29a9\",\"f597d6c4-eb17-11f0-93ba-c64476ca29a9\",\"f748030e-eb17-11f0-93ba-c64476ca29a9\",\"f8a26b22-eb17-11f0-93ba-c64476ca29a9\",\"f8f96ff8-eb17-11f0-93ba-c64476ca29a9\",\"f9a624d2-eb17-11f0-93ba-c64476ca29a9\",\"fa5375ba-eb17-11f0-93ba-c64476ca29a9\",\"ff747120-eb17-11f0-93ba-c64476ca29a9\"]\n",
    "validation_labels = [\"001a711a-eb18-11f0-93ba-c64476ca29a9\",\"2fdcd02e-eb17-11f0-93ba-c64476ca29a9\",\"329177b6-eb17-11f0-93ba-c64476ca29a9\",\"359af54a-eb17-11f0-93ba-c64476ca29a9\",\"486fd3ca-eb17-11f0-93ba-c64476ca29a9\",\"48c5e062-eb17-11f0-93ba-c64476ca29a9\",\"491c5956-eb17-11f0-93ba-c64476ca29a9\",\"4973332a-eb17-11f0-93ba-c64476ca29a9\",\"4acc54ea-eb17-11f0-93ba-c64476ca29a9\",\"60328cec-eb16-11f0-93ba-c64476ca29a9\",\"613aaba6-eb16-11f0-93ba-c64476ca29a9\",\"63a19f94-eb16-11f0-93ba-c64476ca29a9\",\"6651a2ca-eb16-11f0-93ba-c64476ca29a9\",\"6d8ff518-eb17-11f0-93ba-c64476ca29a9\",\"6fef4b56-eb17-11f0-93ba-c64476ca29a9\",\"71f58230-eb17-11f0-93ba-c64476ca29a9\",\"73fd69bc-eb17-11f0-93ba-c64476ca29a9\",\"789f2e38-eb17-11f0-93ba-c64476ca29a9\",\"79a25dc8-eb17-11f0-93ba-c64476ca29a9\",\"7ba98722-eb17-11f0-93ba-c64476ca29a9\",\"7e5c6b9c-eb17-11f0-93ba-c64476ca29a9\",\"80093d3a-eb17-11f0-93ba-c64476ca29a9\",\"8bc179b6-eb18-11f0-93ba-c64476ca29a9\",\"8e1003b4-eb17-11f0-93ba-c64476ca29a9\",\"9063475c-eb17-11f0-93ba-c64476ca29a9\",\"907ccb2c-eb18-11f0-93ba-c64476ca29a9\",\"926b4bf8-eb17-11f0-93ba-c64476ca29a9\",\"94c9dd92-eb17-11f0-93ba-c64476ca29a9\",\"952056e0-eb17-11f0-93ba-c64476ca29a9\",\"ad366292-eb17-11f0-93ba-c64476ca29a9\",\"aee82648-eb17-11f0-93ba-c64476ca29a9\",\"b34cdf9e-eb17-11f0-93ba-c64476ca29a9\",\"b3f96fac-eb17-11f0-93ba-c64476ca29a9\",\"b657ab74-eb17-11f0-93ba-c64476ca29a9\",\"b8b985ae-eb17-11f0-93ba-c64476ca29a9\",\"b90f0178-eb17-11f0-93ba-c64476ca29a9\",\"b9659614-eb17-11f0-93ba-c64476ca29a9\",\"bbc47f4c-eb17-11f0-93ba-c64476ca29a9\",\"bc19ccb8-eb17-11f0-93ba-c64476ca29a9\",\"bcc6ea92-eb17-11f0-93ba-c64476ca29a9\",\"e09c5200-eb16-11f0-93ba-c64476ca29a9\",\"e0f2124e-eb16-11f0-93ba-c64476ca29a9\",\"e1481572-eb16-11f0-93ba-c64476ca29a9\",\"e4ff9a1e-eb16-11f0-93ba-c64476ca29a9\",\"e70ce4b0-eb16-11f0-93ba-c64476ca29a9\",\"e762279a-eb16-11f0-93ba-c64476ca29a9\",\"e80b2be2-eb16-11f0-93ba-c64476ca29a9\",\"eb15bc62-eb16-11f0-93ba-c64476ca29a9\",\"ee1f8988-eb16-11f0-93ba-c64476ca29a9\",\"eeccb6d0-eb16-11f0-93ba-c64476ca29a9\",\"f1253e34-eb16-11f0-93ba-c64476ca29a9\",\"f37f490e-eb16-11f0-93ba-c64476ca29a9\",\"f391611a-eb17-11f0-93ba-c64476ca29a9\",\"f480c4ea-eb16-11f0-93ba-c64476ca29a9\",\"f79f2922-eb17-11f0-93ba-c64476ca29a9\",\"faa90aa2-eb17-11f0-93ba-c64476ca29a9\",\"fe70d908-eb17-11f0-93ba-c64476ca29a9\"]\n",
    "total_labels = [lb for lb in os.listdir('/Applications/Deep Learning/Face Detection/data/labels')]\n",
    "\n",
    "\n",
    "''' \n",
    "Need to take care of the Except Cases\n",
    "'''\n",
    "\n",
    "#Moving labels into corresponding Dataset sub-directory\n",
    "for name in train_labels:\n",
    "    try:\n",
    "        shutil.copy(os.path.join(label_path, name + '.json'), os.path.join(base_path, \"train_set/labels\"))\n",
    "    except:\n",
    "        print(name)\n",
    "        # print('File not found: ', FileNotFoundError())\n",
    "\n",
    "# print(len(os.listdir('/Applications/Deep Learning/Face Detection/data/train_set/labels')))\n",
    "# print(len(os.listdir('/Applications/Deep Learning/Face Detection/data/train_set/images')))\n",
    "\n",
    "\n",
    "# for name in validation_labels:\n",
    "#     try:\n",
    "#         shutil.copy(os.path.join(label_path, name + '.json'), os.path.join(base_path, \"validation_set/labels\"))\n",
    "#     except: #Havent Considered this yet :(\n",
    "#         print(name)\n",
    "\n",
    "\n",
    "# for name in test_labels:\n",
    "#     try:\n",
    "#         shutil.copy(os.path.join(label_path, name + '.json'), os.path.join(base_path, \"test_set/labels\"))\n",
    "#     except:\n",
    "#         print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = \"/Applications/Deep Learning/Face Detection/data/labels\"\n",
    "\n",
    "splits = {\n",
    "    \"train_set\": train_labels,\n",
    "    \"validation_set\": validation_labels,\n",
    "    \"test_set\": test_labels,\n",
    "}\n",
    "\n",
    "missing = []\n",
    "\n",
    "for split, labels in splits.items():\n",
    "    dst = os.path.join(base_path, split, \"labels\")\n",
    "    os.makedirs(dst, exist_ok=True)\n",
    "\n",
    "    for name in labels:\n",
    "        src = os.path.join(label_path, name + \".json\")\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, dst)\n",
    "        else:\n",
    "            missing.append(name)\n",
    "\n",
    "print(f\"Missing label files: {len(missing)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying Image Augmentation on Images and Labels using Albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentor = alb.Compose([alb.RandomCrop(width=900, height=900), #How big Augmented Picture will be\n",
    "                        alb.HorizontalFlip(p=0.5), #\n",
    "                        alb.RandomBrightnessContrast(p=0.2),\n",
    "                        alb.RandomGamma(p=0.2), \n",
    "                        alb.RGBShift(p=0.2), \n",
    "                        alb.VerticalFlip(p=0.5)],\n",
    "                        bbox_params = alb.BboxParams(format='albumentations', label_fields=['class_labels']\n",
    "                        )\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.25, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imread('/Applications/Deep Learning/Face Detection/data/images/00c7b1fe-eb18-11f0-93ba-c64476ca29a9.jpg').shape\n",
    "\n",
    "2.25, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class:  Face\n",
      "1462.5641025641025\n"
     ]
    }
   ],
   "source": [
    "#Load a particuar Image to test\n",
    "train_url = \"/Applications/Deep Learning/Face Detection/data/train_set\"\n",
    "img_example = cv2.imread(\n",
    "    os.path.join(train_url,\"images\", \"00c7b1fe-eb18-11f0-93ba-c64476ca29a9.jpg\")\n",
    ")\n",
    "\n",
    "\n",
    "with open(\n",
    "    '/Applications/Deep Learning/Face Detection/data/train_set/labels/00c7b1fe-eb18-11f0-93ba-c64476ca29a9.json',\n",
    "    \"r\"\n",
    ") as f:\n",
    "    label = json.load(f)\n",
    "\n",
    "#Class of Label\n",
    "print('Class: ', label['shapes'][0]['label'])\n",
    "print(label['shapes'][0]['points'][1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Coordinates [0.30235042735042744, 0.14624881291547953, 0.7617521367521367, 0.8964862298195632]\n"
     ]
    }
   ],
   "source": [
    "h, w = img_example.shape[:2]  # get height and width\n",
    "\n",
    "coordinates = [0, 0, 0, 0]\n",
    "coordinates[0] = label['shapes'][0]['points'][0][0]\n",
    "coordinates[1] = label['shapes'][0]['points'][0][1]\n",
    "coordinates[2] = label['shapes'][0]['points'][1][0]\n",
    "coordinates[3] = label['shapes'][0]['points'][1][1]\n",
    "\n",
    "coordinates = list(np.divide(coordinates, [w,h,w,h]))\n",
    "\n",
    "print('New Coordinates', coordinates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Augmentation and View Results \n",
    "augmented = augmentor(image=img_example, bboxes=[coordinates], class_labels=['face'])\n",
    "print(augmented['image'].shape)\n",
    "\n",
    "\n",
    "# Draw rectangle using normalized coordinates scaled to actual image size\n",
    "cv2.rectangle(\n",
    "    augmented['image'],\n",
    "    tuple((np.array(augmented['bboxes'][0][:2]) * [h,h]).astype(int)),   # top-left\n",
    "    tuple((np.array(augmented['bboxes'][0][2:]) * [h,h]).astype(int)),   # bottom-right\n",
    "    (255, 0, 0), 2\n",
    ")\n",
    "\n",
    "# Show result\n",
    "plt.imshow(cv2.cvtColor(augmented['image'], cv2.COLOR_BGR2RGB))\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run Augmentation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fix our Augmentation cus right now its jibberish\n",
    "'''\n",
    "\n",
    "\n",
    "for partition in ['train_set', 'validation_set', 'test_set']:\n",
    "    for image in os.listdir(os.path.join(base_path, partition, 'images')):\n",
    "        img = cv2.imread(os.path.join(base_path, partition, 'images', image))\n",
    "        coordinates = [0,0,0.00001,0.00001]\n",
    "\n",
    "        #Add Json to name of image\n",
    "        label_path = os.path.join(base_path, partition, 'labels', f'{image.split(\".\")[0]}.json')\n",
    "\n",
    "        #These are the pictures that have been identified as a face\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                label = json.load(f)\n",
    "        \n",
    "            coordinates[0] = label['shapes'][0]['points'][0][0]\n",
    "            coordinates[1] = label['shapes'][0]['points'][0][1]\n",
    "            coordinates[2] = label['shapes'][0]['points'][1][0]\n",
    "            coordinates[3] = label['shapes'][0]['points'][1][1]\n",
    "            h,w = img.shape[:2]\n",
    "            coordinates = list(np.divide(coordinates, [w,h,w,h]))\n",
    "\n",
    "        #Create Annotation for Missing Image Labels and Create 120 Augmented Images\n",
    "    \n",
    "        for x in range(120):\n",
    "            try:\n",
    "                augmented = augmentor(image=img, bboxes=[coordinates], class_labels=['Face'])\n",
    "                cv2.imwrite(os.path.join('/Applications/Deep Learning/Face Detection/aug_data', partition, 'images', f'{image.split(\".\")[0]}.{x}.jpg'), augmented['image'])\n",
    "                annotation = {}\n",
    "                annotation['image'] = f'{image.split(\".\")[0]}.{x}.jpg'\n",
    "\n",
    "                #Image has a label for Face(Classifying Class Labels)\n",
    "                if os.path.exists(label_path):\n",
    "                    if (len(augmented['bboxes']) == 0):\n",
    "                        annotation['bbox'] = [0,0,0,0]\n",
    "                        annotation['class'] = 0\n",
    "                    else:\n",
    "                        annotation['bbox'] = augmented['bboxes'][0]\n",
    "                        annotation['class'] = 1\n",
    "                #That Image Originally did not have label\n",
    "                else:\n",
    "                    annotation['bbox'] = [0,0,0,0]\n",
    "                    annotation['class'] = 0 #Not a Face\n",
    "\n",
    "                #Write these labels\n",
    "                with open(os.path.join('/Applications/Deep Learning/Face Detection/aug_data', partition, 'labels', f'{image.split(\".\")[0]}.{x}.json'), 'w') as f:\n",
    "                    json.dump(annotation, f)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Augmented Images to TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31920\n"
     ]
    }
   ],
   "source": [
    "train_images = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/aug_data/train_set/images/*.jpg', shuffle = False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)\n",
    "\n",
    "print(len(train_images))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/aug_data/test_set/images/*.jpg', shuffle = False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/aug_data/validation_set/images/*.jpg', shuffle = False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120))) #Compress Image\n",
    "val_images = val_images.map(lambda x: x/255) #Scale Image --> Sigmoid Function as all values between [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Label Loading Function\n",
    "def load_labels(label_path):\n",
    "    #Return  the Class in Bounding Box and the Class\n",
    "    with open(label_path.numpy(), 'r', encoding='utf-8') as f:\n",
    "        label = json.load(f)\n",
    "    return [label['class']], label['bbox']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_load_labels(x):\n",
    "    cls, bbox = tf.py_function(\n",
    "        load_labels,\n",
    "        [x],\n",
    "        [tf.uint8, tf.float16]\n",
    "    )\n",
    "\n",
    "    # Explicitly set shapes\n",
    "    cls.set_shape([1])     \n",
    "    bbox.set_shape([4])   # xmin, ymin, xmax, ymax\n",
    "\n",
    "    return cls, bbox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Passed to Load Label Function: b'/Applications/Deep Learning/Face Detection/aug_data/train_set/labels/00717294-eb18-11f0-93ba-c64476ca29a9.0.json'\n",
      "Output of Label Function for Train --> \n",
      "Class:[1] \n",
      "Bounding Box: [0.5493 0.407  1.     0.994 ]\n"
     ]
    }
   ],
   "source": [
    "#Load Labels to Tensorflow Dataset\n",
    "training_labels = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/aug_data/train_set/labels/*.json', shuffle=False)\n",
    "\n",
    "print('Value Passed to Load Label Function:', training_labels.as_numpy_iterator().next()) #Gets Full URL and is passed into load_label function to get corresponding values\n",
    "\n",
    "#Return The Train Label Class and Bounding Box in corresponding format\n",
    "# training_labels = training_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16])) --> tf.py_function caused unknown shape in Tensor\n",
    "training_labels = training_labels.map(\n",
    "    tf_load_labels,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "test_labels = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/aug_data/test_set/labels/*.json', shuffle = False)\n",
    "# test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "test_labels = test_labels.map(\n",
    "    tf_load_labels,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "val_labels = tf.data.Dataset.list_files('/Applications/Deep Learning/Face Detection/aug_data/validation_set/labels/*.json', shuffle = False)\n",
    "# val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))\n",
    "val_labels = val_labels.map(\n",
    "    tf_load_labels,\n",
    "    num_parallel_calls=tf.data.AUTOTUNE\n",
    ")\n",
    "\n",
    "print(f'Output of Label Function for Train --> \\nClass:{training_labels.as_numpy_iterator().next()[0]} \\nBounding Box: {training_labels.as_numpy_iterator().next()[1]}')\n",
    "#tf.py_function allows wrap any Python function into a TF Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Label and Image Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_ParallelMapDataset element_spec=(TensorSpec(shape=(1,), dtype=tf.uint8, name=None), TensorSpec(shape=(4,), dtype=tf.float16, name=None))>\n"
     ]
    }
   ],
   "source": [
    "#Create Final Dataset\n",
    "\n",
    "train = tf.data.Dataset.zip((train_images, training_labels)) #Combine together \n",
    "train = train.shuffle(2046) #2046 not len(train_images) too slow\n",
    "train = train.batch(10000) #Each batch will be 100 Images and 100 Labels\n",
    "train = train.prefetch(4) #Gets images beforehand so when training on certain batch already preprocessing the next\n",
    "\n",
    "\n",
    "print(training_labels)\n",
    "\n",
    "\n",
    "test = tf.data.Dataset.zip((test_images, test_labels)) \n",
    "test = test.shuffle(2046)\n",
    "test = test.batch(10000) \n",
    "test = test.prefetch(4) \n",
    "\n",
    "\n",
    "\n",
    "val = tf.data.Dataset.zip((val_images, val_labels)) #Combine together \n",
    "val = train.shuffle(2046)\n",
    "val  = val.batch(10000) #Each batch will be 100 Images and 100 Labels\n",
    "val = val.prefetch(4) #Gets images beforehand so when training on certain batch already preprocessing the next\n",
    "\n",
    "#train.as_numpy_iterator().next()[0].shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View Images and Annotations\n",
    "data_samples = train.as_numpy_iterator()\n",
    "next_batch = data_samples.next()\n",
    "\n",
    "fig, ax = plt.subplots(ncols=5, nrows=20, figsize=(20,10))\n",
    "\n",
    "for idx in range(5):\n",
    "    sample_image = next_batch[0][idx]\n",
    "    sample_coordinates = next_batch[1][1][idx]\n",
    "\n",
    "    cv2.rectangle(sample_image, tuple(np.multiply(sample_coordinates[:2], [120,120]).astype(int),\n",
    "    tuple(np.multiply(sample_coordinates[2:], [120,120])).astype(int)))\n",
    "\n",
    "    ax[idx].imshow(sample_image)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Deep Learning NN using Functional API\n",
    "\n",
    "1) Classification problem: Identify if its face or not\n",
    "2) Regression: Calculating the Bounding box\n",
    "\n",
    "https://www.geeksforgeeks.org/computer-vision/vgg-16-cnn-model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Base NN \n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten,GlobalMaxPooling2D, Dropout, ReLU\n",
    "from tensorflow.keras.applications import VGG16 #VGG16 Architecture, Pretrained used for Image Classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"vgg16\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"vgg16\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,792</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">147,584</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,168</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │       <span style=\"color: #00af00; text-decoration-color: #00af00\">590,080</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,180,160</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,359,808</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│                                 │ <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                   │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m) │         \u001b[38;5;34m1,792\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m) │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block1_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │       \u001b[38;5;34m147,584\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block2_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m128\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │       \u001b[38;5;34m295,168\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │       \u001b[38;5;34m590,080\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block3_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m256\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m1,180,160\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block4_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv1 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv2 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_conv3 (\u001b[38;5;33mConv2D\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │     \u001b[38;5;34m2,359,808\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ block5_pool (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │             \u001b[38;5;34m0\u001b[0m │\n",
       "│                                 │ \u001b[38;5;34m512\u001b[0m)                   │               │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> (56.13 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m14,714,688\u001b[0m (56.13 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Download VGG16(Classification Model but remove the final activation functions)\n",
    "vgg = VGG16(include_top=False, weights = None)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Info on the Dimension of Boundary:',  train.as_numpy_iterator().next()[1])\n",
    "print('Info regarding Labels:', train.as_numpy_iterator().next()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Custom VG16 Model\n",
    "\n",
    "def new_model():\n",
    "    #Input Dimensions\n",
    "    input_layer = Input(shape=(120,120,3)) \n",
    "\n",
    "    #Pass input_layer as input to Architecture(No Pretrained Weights due to Network issues)\n",
    "    vgg = VGG16(include_top=False, weights=None)(input_layer) \n",
    "\n",
    "    #Classification Problem --> Classify whether Face or Not (Use Sigmoid Activation )\n",
    "    f1 = GlobalMaxPooling2D()(vgg) #Condese all information of the vgg16 and returning max values and will give back 512 values\n",
    "    class1 = Dense(2048, activation='relu')(f1) #Pass F1 and ReLU activation function takes max value and returns 2048 outputs\n",
    "    class2 = Dense(1, activation='sigmoid')(class1) #1 Output value\n",
    "\n",
    "    #Regression Problem(Solve the Optimal Weights to find the coordinates of BBox)\n",
    "    f2 = GlobalMaxPooling2D()(vgg)\n",
    "    regression1 = Dense(2048, activation='relu')(f2)\n",
    "    regression2 = Dense(4, activation='sigmoid')(regression1) #Regress 2 takes 4 Output for each corner of box \n",
    "\n",
    "\n",
    "    face_identifier = Model(inputs=input_layer, outputs = [class2, regression2])\n",
    "    return face_identifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing out the new VGG16 NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>) │ <span style=\"color: #00af00; text-decoration-color: #00af00\">14,714,688</span> │ input_layer_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ vgg16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling2…</span> │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)      │  <span style=\"color: #00af00; text-decoration-color: #00af00\">1,050,624</span> │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │ dense_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)         │      <span style=\"color: #00af00; text-decoration-color: #00af00\">8,196</span> │ dense_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_6       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m3\u001b[0m)                │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ vgg16 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m512\u001b[0m) │ \u001b[38;5;34m14,714,688\u001b[0m │ input_layer_6[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ vgg16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "│ (\u001b[38;5;33mGlobalMaxPooling2…\u001b[0m │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │  \u001b[38;5;34m1,050,624\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)      │  \u001b[38;5;34m1,050,624\u001b[0m │ global_max_pooli… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │      \u001b[38;5;34m2,049\u001b[0m │ dense_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_11 (\u001b[38;5;33mDense\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)         │      \u001b[38;5;34m8,196\u001b[0m │ dense_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,826,181</span> (64.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m16,826,181\u001b[0m (64.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">16,826,181</span> (64.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m16,826,181\u001b[0m (64.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "face_identifier = new_model()\n",
    "face_identifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Shape: (100, 120, 120, 3)\n"
     ]
    }
   ],
   "source": [
    "#Take an Example Sample from Pipeline\n",
    "X,y = train.as_numpy_iterator().next()\n",
    "print('Image Shape:', X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting without Optimization(Need to Fix Prediction Rate: Augmentation Images gone wrong)\n",
    "class_type, predicted_coordinates = face_identifier.predict(X)\n",
    "print('Class Predicted', class_type)\n",
    "print('Bounding Box Predicted Coordinates:', predicted_coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Losses and Optimizers to Evaluate Model\n",
    "\n",
    "* Localization Loss: https://stats.stackexchange.com/questions/319243/object-detection-loss-function-yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How much LR Drops for each epoch:  0.0010416666666666664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Define LR\n",
    "batches_per_epoch = len(train)\n",
    "lr_decay =(1./0.75-1)/batches_per_epoch\n",
    "print('How much LR Drops for each epoch: ', lr_decay)\n",
    "\n",
    "#Define Optimizer(Adam Optimizer)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.00001, decay = lr_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Localization Loss for Bbox and Classification Loss (Binary Class Entropy)\n",
    "\n",
    "def localization_loss(y_true, y_pred):\n",
    "    #Distance between Actual and Predicated Coordinate\n",
    "    delta_coordinate = tf.reduce_sum(tf.square(y_true[:,:2] - y_pred[:,:2]))\n",
    "\n",
    "    #ACTUAL HEIGHT AND WDITH OF BOX (REMEMBER HEIGHT STORED IN [3] AND [1] WHILE WIDTH IS [0] AND [2])\n",
    "    h_true = y_true[:,3] - y_true[:,1]\n",
    "    w_true = y_true[:,2] - y_true[:,0]\n",
    "\n",
    "    #Predicted Height and Wdith of BBox\n",
    "    h_pred = y_pred[:,3] - y_pred[:,1]\n",
    "    w_pred = y_pred[:,2] - y_pred[:,0]\n",
    "\n",
    "    delta_size = tf.reduce_sum(tf.square(w_true - w_pred) + tf.square(h_true - h_pred))\n",
    "\n",
    "    #Localization Loss\n",
    "    return delta_coordinate + delta_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_loss = tf.keras.losses.BinaryCrossentropy()\n",
    "loc_loss = localization_loss #Another way to call our Regression Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test out Loss Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Tensor back to Numpy value\n",
    "print('Testing Sample Localization Loss: ', localization_loss(y[1], predicted_coordinates).numpy()) #y[1] holds our BBox dimensions\n",
    "print('Testing Sample Classification Loss: ', classification_loss(y[0], class_type).numpy()) #y[0] holds the class label\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to Train and Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "Our Model Class for the Neural Network and Training it \n",
    "Uses Super Inheritance\n",
    "'''\n",
    "class FaceModel(Model):\n",
    "    def __init__(self, pre_built_model, **kwargs):\n",
    "        super().__init__(**kwargs) #Super Inheritance\n",
    "        self.model = pre_built_model\n",
    "    \n",
    "    #Used to COnfigure the Model that we will be Training but has not started training yet\n",
    "    def compile(self, optimizer, classification_loss, localization_loss, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.classloss = classification_loss\n",
    "        self.loc_loss = localization_loss\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    #Training the NN and passed the batches of images for it to learn\n",
    "    def train_step(self, batch, **kwargs):\n",
    "        X, y = batch #Split between Images and Labels\n",
    "        # print(y[1])\n",
    "\n",
    "        #Compute all math operations, calculate gradient after forward pass to then do Back propogation\n",
    "        with tf.GradientTape() as tape:\n",
    "            #Pass Images to the Model and it will output classes and BBox for each Image\n",
    "            classes, coordinates = self.model(X, training=True)\n",
    "            batch_classloss = self.classloss(y[0], classes)\n",
    "            batch_locloss = self.loc_loss(tf.cast(y[1], tf.float32), coordinates)\n",
    "\n",
    "            #Can change the 0.5 to a different value if their an anomaly\n",
    "            total_loss = batch_locloss + 0.5 * batch_classloss\n",
    "\n",
    "            #Calculate Gradient --> How to improve the model (Negative Direction is path to Minimize and Magnitude is how much to update Parameter Weights)\n",
    "            gradient = tape.gradient(total_loss, self.model.trainable_variables)\n",
    "\n",
    "        #How our model applys gradient to improve the optimization of the parameter weights\n",
    "        self.optimizer.apply_gradients(zip(gradient, self.model.trainable_variables))\n",
    "\n",
    "        #Store Losses in a Dictionary to access in future\n",
    "        return {'Total loss': total_loss, 'classification_loss':batch_classloss, 'localization_loss': batch_locloss}\n",
    "\n",
    "\n",
    "    #Testing the Model once Training is Complete. How to ensure Model is not Overfitting\n",
    "    def test_step(self, batch, **kwargs):\n",
    "        X, y = batch\n",
    "        classes, coordinates= self.model(X, training = False)\n",
    "\n",
    "        batch_classloss = self.classloss(y[0], classes)\n",
    "        batch_locloss = self.loc_loss(tf.cast(y[1], tf.float32), coordinates)\n",
    "        total_loss = batch_locloss + 0.5 * batch_classloss\n",
    "\n",
    "        #Training over should not need gradients to help train weights, gives final losses for Images in Batch\n",
    "        return {'Total loss': total_loss, 'classification_loss':batch_classloss, 'localization_loss': batch_locloss}\n",
    "\n",
    "\n",
    "    #Necessary if want to use the built in function .predict() \n",
    "    def call(self, X, **kwargs):\n",
    "        return self.model(X, **kwargs, training = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New NN Model after Implementing all functions \n",
    "face_model = FaceModel(face_identifier)\n",
    "\n",
    "face_model.compile(optimizer, classification_loss, loc_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the NN (Saving Logs into a Logs Directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "320\n",
      "Epoch 1/40\n",
      "\u001b[1m 13/320\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:22:18\u001b[0m 40s/step - Total loss: 74.6390 - classification_loss: 0.6925 - localization_loss: 74.2927"
     ]
    }
   ],
   "source": [
    "log_directory = 'logs'\n",
    "#Save all Logs to directory to view for reference and to graph \n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = log_directory)\n",
    "\n",
    "\n",
    "#Training model on n batches if we use train.take(n)\n",
    "hist = face_model.fit(train, epochs = 10, validation_data = val, callbacks = [tensorboard_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=3, figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'], color='red', label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'], color='blue', label='val loss')\n",
    "ax[0].title.set_text('Loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'], color='red', label='class loss')\n",
    "ax[1].plot(hist.history['val_class_loss'], color='blue', label='val class loss')\n",
    "ax[1].title.set_text('Classification Loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regress_loss'], color='red', label='regress loss')\n",
    "ax[2].plot(hist.history['val_regress_loss'], color='blue', label='val regress loss')\n",
    "ax[2].title.set_text('Regression Loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Prediction on Test Set of Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test.as_numpy_iterator()\n",
    "next_test_sample = test_data.next()\n",
    "y_pred = face_model.predict(test_sample[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx in range(4): \n",
    "    sample_image = test_sample[0][idx]\n",
    "    sample_coords = y_pred[1][idx]\n",
    "    \n",
    "    if yhat[0][idx] > 0.9:\n",
    "        cv2.rectangle(sample_image, \n",
    "                      tuple(np.multiply(sample_coords[:2], [120,120]).astype(int)),\n",
    "                      tuple(np.multiply(sample_coords[2:], [120,120]).astype(int)), \n",
    "                            (255,0,0), 2)\n",
    "    \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Model for Future Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_model.save('model.h5')\n",
    "model = load_model('model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
