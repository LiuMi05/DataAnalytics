{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> **Libraries Used** </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_validate\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.metrics import accuracy_score, precision_score, precision_recall_curve, recall_score, f1_score, roc_curve, confusion_matrix, classification_report\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Reading Train and Test Dataset and Seeing missing proportion of each feature</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m df_train = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/Applications/CISC251/home-credit-default-risk (1)/application_train.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m df_test = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/Applications/CISC251/home-credit-default-risk (1)/application_test.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m missing_percent = (\u001b[43mdf_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.sum()/df_train.shape[\u001b[32m0\u001b[39m]) * \u001b[32m100\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(missing_percent)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:6499\u001b[39m, in \u001b[36mDataFrame.isnull\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6494\u001b[39m \u001b[38;5;129m@doc\u001b[39m(NDFrame.isna, klass=_shared_doc_kwargs[\u001b[33m\"\u001b[39m\u001b[33mklass\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   6495\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misnull\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m   6496\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6497\u001b[39m \u001b[33;03m    DataFrame.isnull is an alias for DataFrame.isna.\u001b[39;00m\n\u001b[32m   6498\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/frame.py:6490\u001b[39m, in \u001b[36mDataFrame.isna\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   6488\u001b[39m \u001b[38;5;129m@doc\u001b[39m(NDFrame.isna, klass=_shared_doc_kwargs[\u001b[33m\"\u001b[39m\u001b[33mklass\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   6489\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misna\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> DataFrame:\n\u001b[32m-> \u001b[39m\u001b[32m6490\u001b[39m     res_mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43misna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43misna\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6491\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(res_mgr, axes=res_mgr.axes)\n\u001b[32m   6492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result.__finalize__(\u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33misna\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/base.py:178\u001b[39m, in \u001b[36mDataManager.isna\u001b[39m\u001b[34m(self, func)\u001b[39m\n\u001b[32m    176\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misna\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) -> Self:\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mapply\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/internals/blocks.py:393\u001b[39m, in \u001b[36mBlock.apply\u001b[39m\u001b[34m(self, func, **kwargs)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, **kwargs) -> \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[32m    389\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    390\u001b[39m \u001b[33;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[32m    391\u001b[39m \u001b[33;03m    one\u001b[39;00m\n\u001b[32m    392\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m393\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    395\u001b[39m     result = maybe_coerce_values(result)\n\u001b[32m    396\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._split_op_result(result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:178\u001b[39m, in \u001b[36misna\u001b[39m\u001b[34m(obj)\u001b[39m\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misna\u001b[39m(obj: \u001b[38;5;28mobject\u001b[39m) -> \u001b[38;5;28mbool\u001b[39m | npt.NDArray[np.bool_] | NDFrame:\n\u001b[32m    102\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03m    Detect missing values for an array-like object.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    176\u001b[39m \u001b[33;03m    Name: 1, dtype: bool\u001b[39;00m\n\u001b[32m    177\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:207\u001b[39m, in \u001b[36m_isna\u001b[39m\u001b[34m(obj, inf_as_na)\u001b[39m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    206\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, (np.ndarray, ABCExtensionArray)):\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_isna_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, ABCIndex):\n\u001b[32m    209\u001b[39m     \u001b[38;5;66;03m# Try to use cached isna, which also short-circuits for integer dtypes\u001b[39;00m\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m#  and avoids materializing RangeIndex._values\u001b[39;00m\n\u001b[32m    211\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m obj._can_hold_na:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:292\u001b[39m, in \u001b[36m_isna_array\u001b[39m\u001b[34m(values, inf_as_na)\u001b[39m\n\u001b[32m    290\u001b[39m     result = _isna_recarray_dtype(values, inf_as_na=inf_as_na)\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_string_or_object_np_dtype(values.dtype):\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m     result = \u001b[43m_isna_string_dtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    293\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dtype.kind \u001b[38;5;129;01min\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mmM\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    294\u001b[39m     \u001b[38;5;66;03m# this is the NaT pattern\u001b[39;00m\n\u001b[32m    295\u001b[39m     result = values.view(\u001b[33m\"\u001b[39m\u001b[33mi8\u001b[39m\u001b[33m\"\u001b[39m) == iNaT\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/dtypes/missing.py:313\u001b[39m, in \u001b[36m_isna_string_dtype\u001b[39m\u001b[34m(values, inf_as_na)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m values.ndim \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m         result = \u001b[43mlibmissing\u001b[49m\u001b[43m.\u001b[49m\u001b[43misnaobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[43m=\u001b[49m\u001b[43minf_as_na\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    315\u001b[39m         \u001b[38;5;66;03m# 0-D, reached via e.g. mask_missing\u001b[39;00m\n\u001b[32m    316\u001b[39m         result = libmissing.isnaobj(values.ravel(), inf_as_na=inf_as_na)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/application_train.csv\")\n",
    "df_test = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/application_test.csv\")\n",
    "missing_percent = (df_train.isnull().sum()/df_train.shape[0]) * 100\n",
    "print(missing_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u>**Imputation Techniques --> Preprocessing**</u>\n",
    "- We have many missing values in the dataset.\n",
    "- To create the most informative dataset, we want to retain as much data as possible.\n",
    "- Since we don’t know which features are truly important and may perform feature engineering later, we decided to keep all features rather than dropping them.\n",
    "- Experiments showed that dropping features with >60% missing values led to a significant drop in model performance compared to keeping them.\n",
    "- This suggests that some features, even though they have many missing values, contain critical information for predicting Default vs Non-Default.\n",
    "\n",
    "- **SimpleImputer**:\n",
    "    - Works well when a feature has most of its values present (e.g., >95%).\n",
    "    - Filling missing values with the mean (or median) gives relatively accurate results for such features.\n",
    "###\n",
    "- **IterativeImputer**:\n",
    "    - Predicts missing values for each feature using other features, taking into account their relationships.\n",
    "    - Provides more accurate imputation, especially when features are correlated.\n",
    "    - Caution: It is computationally expensive, so consider runtime when using large datasets. \n",
    "#\n",
    "- **Categorical Multiclass Features**:\n",
    "    - Create a missing flag as these features missing values may be important will be One Hot Encoded in the near future \n",
    "\n",
    "**Recommendation**:\n",
    "- Use SimpleImputer for features with few missing values.\n",
    "- Use IterativeImputer for features where relationships between variables are important and computational cost is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Above 60% Missing values we can drop these features as we dont have enough data to support(Ignore this as there some important features taken out)\n",
    "threshold = 60\n",
    "dropped_features = missing_percent[missing_percent > threshold].index\n",
    "print(\"*****************************\")\n",
    "print(\"Dropped Features(60% missing values): \", dropped_features.tolist())\n",
    "df_train = df_train.drop(columns=dropped_features)\n",
    "print(\"*****************************\")\n",
    "print(\"Kept Features:\", df_train.columns.tolist())\n",
    "print(\"*****************************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Dropped Features(60% missing values):  ['OWN_CAR_AGE', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'FLOORSMIN_AVG', 'LIVINGAPARTMENTS_AVG', 'NONLIVINGAPARTMENTS_AVG', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'FLOORSMIN_MODE', 'LIVINGAPARTMENTS_MODE', 'NONLIVINGAPARTMENTS_MODE', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'FLOORSMIN_MEDI', 'LIVINGAPARTMENTS_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'FONDKAPREMONT_MODE']\n",
    "\n",
    "Of these Features one of them is significantly important in testing for Default vs Non Default do an analysis on the features to determine which one is a pivotal feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SimpleImputer on Features with Missing values less than 5%\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "threshold = 5\n",
    "features_simple_impute = missing_percent[(missing_percent <= threshold) & (missing_percent != 0) ].index\n",
    "\n",
    "simple_imputer = SimpleImputer(strategy=\"median\")\n",
    "simple_imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "for column in features_simple_impute:\n",
    "\n",
    "    if (df_train[column].dtype != \"object\"):\n",
    "        df_train[column] = simple_imputer.fit_transform(df_train[[column]]).ravel()\n",
    "        \n",
    "    else:\n",
    "        df_train[column] = simple_imputer_cat.fit_transform(df_train[[column]]).ravel() #Removes one Dimension using ravel (3,3) to (3,)\n",
    "\n",
    "\n",
    "print(\"Features that have been Univariate Imputation with Median or Most Frequent: \", features_simple_impute.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns that have been Iteratively Imputed:  ['OWN_CAR_AGE', 'EXT_SOURCE_1', 'EXT_SOURCE_3', 'APARTMENTS_AVG', 'BASEMENTAREA_AVG', 'YEARS_BEGINEXPLUATATION_AVG', 'YEARS_BUILD_AVG', 'COMMONAREA_AVG', 'ELEVATORS_AVG', 'ENTRANCES_AVG', 'FLOORSMAX_AVG', 'FLOORSMIN_AVG', 'LANDAREA_AVG', 'LIVINGAPARTMENTS_AVG', 'LIVINGAREA_AVG', 'NONLIVINGAPARTMENTS_AVG', 'NONLIVINGAREA_AVG', 'APARTMENTS_MODE', 'BASEMENTAREA_MODE', 'YEARS_BEGINEXPLUATATION_MODE', 'YEARS_BUILD_MODE', 'COMMONAREA_MODE', 'ELEVATORS_MODE', 'ENTRANCES_MODE', 'FLOORSMAX_MODE', 'FLOORSMIN_MODE', 'LANDAREA_MODE', 'LIVINGAPARTMENTS_MODE', 'LIVINGAREA_MODE', 'NONLIVINGAPARTMENTS_MODE', 'NONLIVINGAREA_MODE', 'APARTMENTS_MEDI', 'BASEMENTAREA_MEDI', 'YEARS_BEGINEXPLUATATION_MEDI', 'YEARS_BUILD_MEDI', 'COMMONAREA_MEDI', 'ELEVATORS_MEDI', 'ENTRANCES_MEDI', 'FLOORSMAX_MEDI', 'FLOORSMIN_MEDI', 'LANDAREA_MEDI', 'LIVINGAPARTMENTS_MEDI', 'LIVINGAREA_MEDI', 'NONLIVINGAPARTMENTS_MEDI', 'NONLIVINGAREA_MEDI', 'TOTALAREA_MODE', 'AMT_REQ_CREDIT_BUREAU_HOUR', 'AMT_REQ_CREDIT_BUREAU_DAY', 'AMT_REQ_CREDIT_BUREAU_WEEK', 'AMT_REQ_CREDIT_BUREAU_MON', 'AMT_REQ_CREDIT_BUREAU_QRT', 'AMT_REQ_CREDIT_BUREAU_YEAR']\n"
     ]
    }
   ],
   "source": [
    "#Run IterativeImputer on all other Columns that are numeric and have missing values\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "iterative_imputer = IterativeImputer(random_state = 0)\n",
    "numeric_missing_columns = [ column for column in df_train.select_dtypes(include=[\"float64\", \"int64\"]).columns \n",
    "                                if df_train[column].isnull().sum() > 0]\n",
    "\n",
    "df_train[numeric_missing_columns] = pd.DataFrame(iterative_imputer.fit_transform(df_train[numeric_missing_columns]), columns=numeric_missing_columns, index=df_train.index)\n",
    "print(\"Columns that have been Iteratively Imputed: \", numeric_missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Categorical Features with higher than 5% Missing Values assign a new Column called Missing_{Column_name}_Flag\n",
    "columns = df_train.columns[(df_train.isnull().sum() > 0)]\n",
    "\n",
    "#Creating Missing Flag for each Column Categorical with more than 5% Missing Values\n",
    "for col in columns:\n",
    "    df_train[col + \"_missing_flag\"] = df_train[col].isnull().astype(int)\n",
    "\n",
    "for col in columns:\n",
    "    if df_train[col].dtype == \"object\":\n",
    "        df_train[col] = df_train[col].fillna(\"Missing\")\n",
    "print(\"Categorical Columns that have been assigned a new class called Missing: \", columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Check that all Columns in df_train have been imputed meaning no more missing values</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(df_train.isnull().sum().sum() == 0): \n",
    "    print(\"All Columns have no missing values\")\n",
    "else:\n",
    "    print(\"Columns still have missing values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Creating Train, Validation, Test Set for Base Model(KNN, LOGREG, FOREST MODELS)</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(columns=\"TARGET\")\n",
    "y = df_train[\"TARGET\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train, y_train, test_size = 0.1/(1-0.3), stratify=y_train, random_state=42)\n",
    "le = LabelEncoder()\n",
    "for column in X_train.columns:\n",
    "    if X_train[column].dtype == \"object\":\n",
    "        le.fit(X_train[column]) \n",
    "        X_train[column] = le.transform(X_train[column])\n",
    "        X_val[column] = le.transform(X_val[column])\n",
    "        X_test[column] = le.transform(X_test[column])\n",
    "        \n",
    "#Run Autogun Algorithm to find best models\n",
    "autoflag = input(\"Run Autogun:(Y/N): \")\n",
    "if(autoflag == 'Y' or autoflag == 'y' ):\n",
    "    predictor = TabularPredictor(label='TARGET').fit(train_data=df_train)\n",
    "    predictions = predictor.predict(df_test)\n",
    "    print(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Initiation for SMOTE(Oversampling Method ) and StratifiedKFold Cross Validation</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold, cross_val_score, cross_validate\n",
    "%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits = 10, shuffle=True, random_state = 42)\n",
    "sm = SMOTE(random_state = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>BASE KNN Model</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale only Numeric Values ---> KNN needs this\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "knn_Classifier = KNeighborsClassifier()\n",
    "print(X_train.shape[0], y_train.shape[0])\n",
    "knn_Classifier.fit(X_train, y_train)  #Use Default Hyperparmas\n",
    "y_pred_val = knn_Classifier.predict(X_val)\n",
    "y_pred_train = knn_Classifier.predict(X_train)\n",
    "y_pred_test = knn_Classifier.predict(X_test)\n",
    "\n",
    "print(\"KNN Accuracy Train Score:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"KNN Precision Train Score: \", precision_score(y_train, y_pred_train))\n",
    "print(\"KNN Recall Train Score: \", recall_score(y_train, y_pred_train))\n",
    "print(\"KNN F1 Train Score: \", f1_score(y_train, y_pred_train))\n",
    "\n",
    "print(\"KNN Accuracy Validation Score:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"KNN Precision Validation Score: \", precision_score(y_val, y_pred_val))\n",
    "print(\"KNN Recall Validation Score: \", recall_score(y_val, y_pred_val))\n",
    "print(\"KNN F1 Validation Score: \", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"KNN Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"KNN Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"KNN Recall Test Score: \", recall_score(y_test, y_pred_test)) #Super low means that is overfitting\n",
    "print(\"KNN F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train and Test]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = \"KNN Classifier\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of KNN\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = \"KNN Classifier\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of KNN\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=knn_Classifier, X = X_train, y = y_train, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\"], return_train_score=True)\n",
    "for metric in [\"train_accuracy\", \"train_precision\", \"train_recall\", \"test_accuracy\", \"test_precision\", \"test_recall\"]:\n",
    "    mean_score = np.mean(scores[metric])\n",
    "    print(f\"Mean {metric}: \", mean_score)\n",
    "    print(\"STD: \", np.std(scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Random Forest Tree Base Model</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_val = rf_model.predict(X_val)\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "print(\"RF Accuracy Train Score:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"RF Precision Train Score: \", precision_score(y_train, y_pred_train))\n",
    "print(\"RF Recall Train Score: \", recall_score(y_train, y_pred_train))\n",
    "print(\"RF F1 Train Score: \", f1_score(y_train, y_pred_train))\n",
    "\n",
    "print(\"RF Accuracy Validation Score:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"RF Precision Validation Score: \", precision_score(y_val, y_pred_val))\n",
    "print(\"RF Recall Validation Score: \", recall_score(y_val, y_pred_val))\n",
    "print(\"RF F1 Validation Score: \", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"RF Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"RF Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"RF Recall Test Score: \", recall_score(y_test, y_pred_test)) #Super low means that is overfitting\n",
    "print(\"RF F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train and Test and Validation]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = \"RF Classifier\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of RF\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr, tpr, color=\"red\", label = f\"ROC CURVE for RF Validation with {roc_auc} score\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", label=f\"ROC CURVE Random model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"RF ROC CURVE for Validation vs Random Model\")\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = \"RF Classifier\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of RF\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('Feature Importance\\:\\n', pd.Series(rf_model.feature_importances_, index = X.columns))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=rf_model, X = X_train, y = y_train, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\"], return_train_score=True)\n",
    "for metric in [\"train_accuracy\", \"train_precision\", \"train_recall\", \"test_accuracy\", \"test_precision\", \"test_recall\"]:\n",
    "    mean_score = np.mean(scores[metric])\n",
    "    print(f\"Mean {metric}: \", mean_score)\n",
    "    print(\"STD: \", np.std(scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>XGBoost Classifier Base Mode</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost Classifier\n",
    "%pip install xgboost\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "xgb_model = xgb.XGBClassifier(ignore_coerce = True)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_val = xgb_model.predict(X_val)\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGBModel Accuracy Train Score:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"XGBModel Precision Train Score: \", precision_score(y_train, y_pred_train))\n",
    "print(\"XGBModel Recall Train Score: \", recall_score(y_train, y_pred_train))\n",
    "print(\"XGBModel F1 Train Score: \", f1_score(y_train, y_pred_train))\n",
    "\n",
    "print(\"XGB Accuracy Validation Score:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"XGB Precision Validation Score: \", precision_score(y_val, y_pred_val))\n",
    "print(\"XGB  Recall Validation Score: \", recall_score(y_val, y_pred_val))\n",
    "print(\"XGB F1 Validation Score: \", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"XGB Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"XGB Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"XGB Recall Test Score: \", recall_score(y_test, y_pred_test)) \n",
    "print(\"XGB F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train and Test]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = \"XGB Classifier\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of XGB\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = \"XGB Classifier\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of XGB\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=xgb_model, X = X_train, y = y_train, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\"], return_train_score=True)\n",
    "for metric in [\"train_accuracy\", \"train_precision\", \"train_recall\", \"test_accuracy\", \"test_precision\", \"test_recall\"]:\n",
    "    mean_score = np.mean(scores[metric])\n",
    "    print(f\"Mean {metric}: \", mean_score)\n",
    "    print(\"STD: \", np.std(scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Base Model Logistic Regressio</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, accuracy_score, auc, f1_score, recall_score\n",
    "\n",
    "\n",
    "\n",
    "log_model = LogisticRegression(random_state = 42, class_weight=balanced)\n",
    "log_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_val = log_model.predict(X_val)\n",
    "y_pred_train = log_model.predict(X_train)\n",
    "y_pred_test = log_model.predict(X_test)\n",
    "\n",
    "print(\"Log Reg Accuracy Train Score:\", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Log Reg Precision Train Score: \", precision_score(y_train, y_pred_train))\n",
    "print(\"Log Reg Recall Train Score: \", recall_score(y_train, y_pred_train))\n",
    "print(\"Log Reg F1 Train Score: \", f1_score(y_train, y_pred_train))\n",
    "\n",
    "print(\"Log Reg Accuracy Validation Score:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"Log Reg Precision Validation Score: \", precision_score(y_val, y_pred_val))\n",
    "print(\"Log Reg  Recall Validation Score: \", recall_score(y_val, y_pred_val))\n",
    "print(\"Log Reg F1 Validation Score: \", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"Log Reg Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Log Reg Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"Log Reg Recall Test Score: \", recall_score(y_test, y_pred_test))\n",
    "print(\"Log Reg F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train, Validation and Test]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train, y_pred_train)\n",
    "roc_score = roc_auc_score(y_train, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of LogReg\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_score = roc_auc_score(y_val, y_pred_val)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Validation of LogReg\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with AUC score: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of XGB\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#ROC Score is below 0.5 meaning that is predicting worse than model do 1 - pred and if it jumps above 0.5 thats good, that was not case dont use Log Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=log_model, X = X_train, y = y_train, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\"], return_train_score=True)\n",
    "for metric in [\"train_accuracy\", \"train_precision\", \"train_recall\", \"test_accuracy\", \"test_precision\", \"test_recall\"]:\n",
    "    mean_score = np.mean(scores[metric])\n",
    "    print(f\"Mean {metric}:\", mean_score)\n",
    "    print(\"STD:\", np.std(scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Base Model LightGMB Classifier</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet lightgbm\n",
    "import lightgbm \n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, precision_score, accuracy_score, auc, f1_score, recall_score\n",
    "\n",
    "#Keep this Model --> Treebased can handle without SMOTE\n",
    "lgb_model = LGBMClassifier(verbosity=-1) #Surpass the messages\n",
    "lgb_model.fit(X_train,y_train)\n",
    "print(\"Parameters of Baseline Model:\", lgb_model.get_params())\n",
    "\n",
    "\n",
    "y_pred_train = lgb_model.predict(X_train)\n",
    "y_pred_val = lgb_model.predict(X_val)\n",
    "y_pred_test = lgb_model.predict(X_test)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train, y_pred_train))\n",
    "print(\"Train Precision: \", precision_score(y_train, y_pred_train))\n",
    "print(\"Train Recall:\", recall_score(y_train, y_pred_train))\n",
    "print(\"F1 Score:\", f1_score(y_train, y_pred_train))\n",
    "\n",
    "\n",
    "print(\"Val Accuracy: \", accuracy_score(y_val, y_pred_val))\n",
    "print(\"Val Precision: \", precision_score(y_val, y_pred_val))\n",
    "print(\"Val Recall:\", recall_score(y_val, y_pred_val))\n",
    "print(\"Val F1 Score:\", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"Recall Test Score: \", recall_score(y_test, y_pred_test))\n",
    "print(\"F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train, Validation and Test]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train, y_pred_train)\n",
    "roc_score = roc_auc_score(y_train, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = f\"LGB with ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of LGBClassifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_score = roc_auc_score(y_val, y_pred_val)\n",
    "plt.plot(fpr, tpr, label = f\"LGB with ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Validation of LGBClassifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with AUC score: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of LGBClassifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(estimator=lgb_model, X = X_train, y = y_train, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\"], return_train_score=True)\n",
    "for metric in [\"train_accuracy\",  \"train_precision\", \"train_recall\", \"test_accuracy\", \"test_precision\", \"test_recall\"]:\n",
    "    mean_score = np.mean(scores[metric])\n",
    "    print(f\"Mean {metric}: \", mean_score)\n",
    "    print(\"STD: \", np.std(scores[metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Improvement to Base Model ---> Label Encoding and One Hot Encoding to Encode Categorical Data**</u>\n",
    "\n",
    "\n",
    "**Label Encoding**:\n",
    "- Converts categorical features into ordinal integers.\n",
    "- Useful for features with two categories (binary).\n",
    "- Yes → 1, No → 0.\n",
    "\n",
    "**One-Hot Encoding**:\n",
    "- Creates a separate column for each unique category in a feature.\n",
    "- Typically used when the feature is non-binary.\n",
    "\n",
    "**Issues with One-Hot Encoding**:\n",
    "- When a feature has multiple unique categories, the number of dimensions (features) can become extremely large like 100,000 which is R^(100,000) vector plane\n",
    "- Each data point becomes a high-dimensional sparse vector, mostly zeros with a single one.\n",
    "\n",
    "**Problems with High Dimension**\n",
    "- 1. Memory usage and computation cost increase dramatically.\n",
    "- 2. High-dimensional sparse data makes the model prone to overfitting. The model may memorize training data rather than learn meaningful patterns.\n",
    "- 3. As the vectors are nearly/almost unique, the model can separate training points easily but struggles to generalize predicting unseen data like our test set which means the model is not learning meaningful patterns from the train set but instead memorizing the shape of train_set so when it comes to the test_set it will try fit the test_set on the train_set but these two sets differ astronomically\n",
    "\n",
    "**Summary**:\n",
    "- We use Label Encoding for Binary Features and One Hot Encoding for Multiclass features. The reason why at start at notebook we did Label Encoding for all was because we did not care much for the importance of features as a base line model is one that uses RAW data with no transformations or anything. This is to give us an indication on how well a Random Model would do with the data provided. We do not care much for data preprocessing as this is done usually when we want to improve a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Note tree models dont need to do this as they can handle ordinal data\n",
    "'''\n",
    "#Run LabelEncoder on Binary Categorical Features\n",
    "le = LabelEncoder()\n",
    "\n",
    "#Run OneHotEncoder on Categorical Features with 2+ Classes\n",
    "ohe = OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)\n",
    "\n",
    "for column in df_train.columns:\n",
    "\n",
    "    if df_train[column].dtype == \"object\" and df_train[column].nunique() == 2:\n",
    "        le.fit(df_train[column]) #Fit the Data to LabelEncoder to Train Data prevent Data leakage\n",
    "        df_train[column] = le.transform(df_train[column])\n",
    "        df_test[column] = le.transform(df_test[column])\n",
    "\n",
    "    elif df_train[column].dtype == \"object\" and df_train[column].value_counts().nunique() > 2:\n",
    "        ohe.fit(df_train[[column]]) #Expects 2D[n_samples, n_features] df_train[column] is 1D Series\n",
    "        #Returns Arrays need convert to DF\n",
    "        train_encoded = ohe.transform(df_train[[column]])\n",
    "        test_encoded = ohe.transform(df_test[[column]])\n",
    "        encoded_cols = ohe.get_feature_names_out([column])\n",
    "        #New DataFrames Created with Encoding\n",
    "        df_train_encoded = pd.DataFrame(train_encoded, columns=encoded_cols, index = df_train.index)\n",
    "        df_test_encoded = pd.DataFrame(test_encoded, columns = encoded_cols, index = df_test.index)\n",
    "\n",
    "        #Drop Original Dataframes\n",
    "        df_train = pd.concat([df_train.drop(columns=column), df_train_encoded], axis = 1)\n",
    "        df_test = pd.concat([df_test.drop(columns=column), df_test_encoded], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> **Base Model Improvement 2 ----> SMOTE**</u>\n",
    "\n",
    "**Issue**:\n",
    " - The model is overfitting due to class imbalance. The majority class (Non-default) makes up ~90% of the data, while the minority class (Default) is only ~10%. This causes the model to memorize patterns from the majority class and perform poorly on the minority class.\n",
    "\n",
    "**Why it matters**:\n",
    "- Accuracy is misleading for imbalanced datasets because the model can appear highly accurate by only predicting the majority class. With so few minority samples, the model cannot learn meaningful patterns for the rare class.\n",
    "\n",
    "**Solution – SMOTE (Synthetic Minority Oversampling Technique):**\n",
    "- SMOTE generates synthetic samples for the minority class by interpolating between existing minority class instances.\n",
    "- This increases the representation of the minority class without losing any majority class data (unlike undersampling).\n",
    "- By balancing the classes, the model can learn patterns from both classes, improving its performance on the minority class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE RETURNS Numpy Array \n",
    "%pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state = 100)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train) #X_shape: (121,150) to (229,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> **Base Model Improvement 3 ----> PCA[Linear Models like Logisitic Regression]**</u>\n",
    "\n",
    "- Use PCA (dimensionality reduction technique) to help with issues caused by one-hot encoding. One-hot encoding often creates many sparse and correlated features, which can make modeling harder. PCA finds linear overlap among features and groups similar features together, while preserving as much of the original information as possible (variance).\n",
    "\n",
    "How PCA works:\n",
    "- Some dimensions carry more meaningful variation between points, representing strong patterns in the data, while others carry less meaningful variation, often representing noise rather than signal. PCA identifies directions (axes) in the feature space that have the most variation. These directions become the new principal components. It rotates the original coordinate system to align with these directions of maximum variance, so the new components capture the most important differences between points. Each principal component is a linear combination of the original features, which means that even after reducing dimensionality, we are still keeping as much information as possible from the original dataset. Also for Logistical Regression this works as features are still linear just linear_combination of features they are non nonlinear\n",
    "\n",
    "- Why this is useful:\n",
    "    - Reduces dimensionality and simplifies the model without losing key information(Captures most meaningful patterns while being robust)\n",
    "    - Groups correlated features together, which prevents redundancy from one-hot encoding(Removes Dimensionality which makes it less Computationally Expensive)\n",
    "    - Helps the model focus on the most informative patterns, improving stability and interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE USED TO FIND THE PERFECT AMOUNT OF PC to explain 95% Variance -> 68\n",
    "scaler = StandardScaler()\n",
    "flag_pca = input(\"Do you want to run PCA to improve Model(Y/N): \")\n",
    "if(flag_pca == \"Y\" or flag_pca == \"y\"):\n",
    "    flag_1 = False\n",
    "    X_train_scaled = scaler.fit_transform(X_train_smote)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    if flag_1:\n",
    "        pca = PCA().fit(X_train_scaled)\n",
    "        cumulative_variance = pca.explained_variance_ratio_.cumsum() \n",
    "        plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
    "        plt.xlabel('Number of components')\n",
    "        plt.ylabel('Cumulative explained variance')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    pca = PCA(n_components = 0.95) #can explain 0.95 of variance or n_components = 75\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    X_val_pca = pca.transform(X_val_scaled)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "\n",
    "    # print(\"Cumulative Variance:\", pca.explained_variance_ratio_.cumsum())\n",
    "    # pc_variance = pca.explained_variance_ratio_\n",
    "    # print(\"Each Individual PC Variance Explained: \", pc_variance)\n",
    "\n",
    "    #Keep Threshold amount until Cumulative Variance is 95%[Number of PC to Keep]\n",
    "    pc_variance = pca.explained_variance_ratio_\n",
    "    cumulative_variance = np.cumsum(pc_variance)\n",
    "    n_components = np.argmax(cumulative_variance >= 0.95) + 1 #Returns First True value and then also +1 includes that PC without it it will exclude it\n",
    "    print(\"Number of Components to keep for 95% variance: \", n_components)\n",
    "    X_pca = pca.transform(X_train_scaled)[:,:n_components] #Shape: (229, n_components)\n",
    "\n",
    "    df_pca = pd.DataFrame(X_pca, columns=[f\"PC{i+1}\" for i in range(n_components)])\n",
    "    print(\"Transformed PCA DF Shape:\", df_pca.shape)\n",
    "        \n",
    "    #pca.components_ --> (n_components_total, n_features) where each row is a PC and each column a feature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u> **Feature Contribution for each PC** </u>\n",
    "\n",
    "- We have 74 principal components (PCs) that together explain 95% of the variance in our dataset. Each PC is significantly important, as all of them contribute to building this 95% variance. PC1 explains the most variance, while PC74 explains the least. However, since each PC is calculated based on all features, removing a single feature can potentially change the composition of all PCs.\n",
    "\n",
    "- Loadings are used to describe how much each original feature contributes to a given PC. We have created a dataframe to help users visualize which features contribute the most to each PC, showing which features describe each PC the most effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tabulate\n",
    "from tabulate import tabulate\n",
    "loading = pd.DataFrame(pca.components_[:n_components,:].T,\n",
    "                    columns=[f\"PC{i+1}\" for i in range(n_components)],\n",
    "                    index = X.columns\n",
    "                    )\n",
    "\n",
    "print(tabulate(loading, headers=\"keys\", tablefmt=\"grid\"))\n",
    "print(\"Feature that contribute most to each PC: \", [(i, loading[i].idxmax()) for i in loading.columns])\n",
    "most_import_feature = [(i, loading[i].idxmax()) for i in loading.columns]\n",
    "df_importance_pc = pd.DataFrame(most_import_feature, columns=[\"PC\", \"Feature\"])\n",
    "print(tabulate(df_importance_pc, headers=\"keys\", tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> **Decision Boundary for PC1 and PC2 of Logistic Regression Model** </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions  \n",
    "plt.figure(figsize=(10,8))\n",
    "plot_decision_regions(X_train_pca[[\"PC1\", \"PC2\"]], np.array([y_train_smote]).ravel(), clf=log_model)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **<u> Improvement on Base Model Logistic Regression with SMOTE and PCA</u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "print(\"*************************************************\")\n",
    "print(\"Same Shape: \",(X_train_pca.shape, y_train_smote.shape, True if X_pca.shape[0] == y_train_smote.shape[0] else False) )\n",
    "print(\"*************************************************\")\n",
    "log_model = log_model.fit(X_train_pca, y_train_smote)\n",
    "\n",
    "#PCA Reuced our total Features to 69\n",
    "y_pred_train = log_model.predict(X_train_pca)\n",
    "y_pred_val = log_model.predict(X_val_pca)\n",
    "y_pred_test = log_model.predict(X_test_pca)\n",
    "\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_smote, y_pred_train))\n",
    "print(\"Train Precision: \", precision_score(y_train_smote, y_pred_train))\n",
    "print(\"Train Recall:\", recall_score(y_train_smote, y_pred_train))\n",
    "print(\"F1 Score:\", f1_score(y_train_smote, y_pred_train))\n",
    "print(\"*************************************************\")\n",
    "\n",
    "print(\"Val Accuracy: \", accuracy_score(y_val, y_pred_val))\n",
    "print(\"Val Precision: \", precision_score(y_val, y_pred_val))\n",
    "print(\"Val Recall:\", recall_score(y_val, y_pred_val))\n",
    "print(\"Val F1 Score:\", f1_score(y_val, y_pred_val))\n",
    "print(\"*************************************************\")\n",
    "\n",
    "print(\"Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"Recall Test Score: \", recall_score(y_test, y_pred_test))\n",
    "print(\"F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "print(\"*************************************************\")\n",
    "\n",
    "\n",
    "#Create a ROC Curve[For Train, Validation and Test]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train_smote, y_pred_train)\n",
    "roc_score = roc_auc_score(y_train_smote, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of Log Reg Classifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_score = roc_auc_score(y_val, y_pred_val)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Validation of Log Reg Classifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE AUC score: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of Log Reg Classifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<u>LOG REG WITH SMOTE NO PCA </u>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "print(\"*************************************************\")\n",
    "print(\"Same Shape: \",(X_train_smote.shape, y_train_smote.shape, True if X_train_smote.shape[0] == y_train_smote.shape[0] else False) )\n",
    "print(type(y_train_smote))\n",
    "print(y_train_smote.value_counts())\n",
    "print(\"*************************************************\")\n",
    "log_model = log_model\n",
    "log_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "#PCA Reuced our total Features to 69\n",
    "y_pred_train = log_model.predict(X_train_smote)\n",
    "y_pred_val = log_model.predict(X_val)\n",
    "y_pred_test = log_model.predict(X_test)\n",
    "\n",
    "#Average = \"macro\" calculates the evaluation metric independently for each class and takes the unweighted average --> very good for balanced classes(each class contributed equally)\n",
    "print(\"Train Accuracy: \", accuracy_score(y_train_smote, y_pred_train))\n",
    "print(\"Train Precision: \", precision_score(y_train_smote, y_pred_train, average=\"macro\"))\n",
    "print(\"Train Recall:\", recall_score(y_train_smote, y_pred_train, average=\"macro\"))\n",
    "print(\"F1 Score:\", f1_score(y_train_smote, y_pred_train, average=\"macro\"))\n",
    "print(\"*************************************************\")\n",
    "\n",
    "print(\"Val Accuracy: \", accuracy_score(y_val, y_pred_val))\n",
    "print(\"Val Precision: \", precision_score(y_val, y_pred_val, average=\"macro\"))\n",
    "print(\"Val Recall:\", recall_score(y_val, y_pred_val, average=\"macro\"))\n",
    "print(\"Val F1 Score:\", f1_score(y_val, y_pred_val, average=\"macro\"))\n",
    "print(\"*************************************************\")\n",
    "\n",
    "print(\"Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"Precision Test Score: \", precision_score(y_test, y_pred_test, average=\"macro\"))\n",
    "print(\"Recall Test Score: \", recall_score(y_test, y_pred_test, average=\"macro\"))\n",
    "print(\"F1 Test Score: \", f1_score(y_test, y_pred_test, average=\"macro\"))\n",
    "print(\"*************************************************\")\n",
    "\n",
    "\n",
    "#Create a ROC Curve[For Train, Validation and Test]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train_smote, y_pred_train)\n",
    "roc_score = roc_auc_score(y_train_smote, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of Log Reg Classifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_score = roc_auc_score(y_val, y_pred_val)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE ROC Score:{roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Validation of Log Reg Classifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE AUC score: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of Log Reg Classifier\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>**Stratified KFold Cross Validation**</u>\n",
    "\n",
    "- We use cross-validation (CV) to evaluate the model’s general performance, rather than relying on a single train-test split that could be unusually good or bad. For each validation split, we calculate the performance metrics, then take the average across all splits to get a robust estimate. We also compare training and testing metrics to detect overfitting. Additionally, we compute the standard deviation of the metrics across splits to assess variability, ensuring that performance is consistent and stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_validate\n",
    "pca_allowed = input(\"Does your Log Reg Model have PCA(Y/N): \")\n",
    "if pca_allowed == \"Y\" or pca_allowed == \"y\":\n",
    "    scores = cross_validate(estimator=log_model, X = X_train_pca, y = y_train_smote, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\", \"roc_auc\"], return_train_score=True)\n",
    "    for metric in [\"train_accuracy\", \"train_precision\", \"train_recall\", \"train_roc_auc\", \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_roc_auc\"]:\n",
    "        mean_score = np.mean(scores[metric])\n",
    "        print(f\"Mean {metric}: \", mean_score)\n",
    "        print(\"STD: \", np.std(scores[metric]))\n",
    "else:\n",
    "    scores = cross_validate(estimator=log_model, X = X_train_smote, y = y_train_smote, cv = kfold, scoring=[\"accuracy\", \"precision\", \"recall\", \"roc_auc\"], return_train_score=True)\n",
    "    for metric in [\"train_accuracy\", \"train_precision\", \"train_recall\", \"train_roc_auc\", \"test_accuracy\", \"test_precision\", \"test_recall\", \"test_roc_auc\"]:\n",
    "        mean_score = np.mean(scores[metric])\n",
    "        print(f\"Mean {metric}: \", mean_score)\n",
    "        print(\"STD: \", np.std(scores[metric]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> **Testing it on the Final Model**</u> \n",
    "- We have data leakage as we performed SMOTE before Validation so its actually like overfitting\n",
    "- WE must tune SMOTE parameters, LogisticRegression parameters, PCA parameters, Model feature engineering and selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Set Results:\n",
      "Accuracy: 0.7886\n",
      "Precision: 0.1580\n",
      "Recall: 0.3739\n",
      "ROC AUC: 0.6541\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, X_set, y_set in [\n",
    "        (\"Test\", X_test_pca, y_test)\n",
    "    ]:\n",
    "        y_pred = log_model.predict(X_set)\n",
    "        y_pred_proba = log_model.predict_proba(X_set)[:, 1]\n",
    "\n",
    "        print(f\"\\n{name} Set Results:\")\n",
    "        print(f\"Accuracy: {accuracy_score(y_set, y_pred):.4f}\")\n",
    "        print(f\"Precision: {precision_score(y_set, y_pred):.4f}\")\n",
    "        print(f\"Recall: {recall_score(y_set, y_pred):.4f}\")\n",
    "        print(f\"ROC AUC: {roc_auc_score(y_set, y_pred_proba):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>**Classification Reports**</u>\n",
    "\n",
    "- By default, the Classification Report returns a string.\n",
    "\n",
    "- Our evaluation metrics typically treat the true positive class as the only positive class. However, the classification report can evaluate both classes as the positive class in separate scenarios.\n",
    "\n",
    "- The classification report allows us to see both macro and weighted metrics:\n",
    "  - Macro assumes all classes are equally important (useful for balanced datasets).\n",
    "  - Weighted accounts for class imbalance by weighting metrics according to the number of samples in each class.\n",
    "####\n",
    "- Note: Even if our true labels are balanced using SMOTE, this does not guarantee that predictions will be balanced. This is why macro and weighted metrics can differ.\n",
    "\n",
    "- Using cross-validation, specifically Stratified K-Fold CV, allows us to observe how the classification report metrics vary (or remain consistent) across folds. This helps determine if the model is genuinely performing well or if high performance on a particular fold is just luck, potentially indicating overfitting.\n",
    "\n",
    "- Choosing the optimal k in K-Fold CV is important and can be done via GridSearchCV. For example, using k = 40 produced similar results in my experiments.\n",
    "  - A low k introduces high bias but is faster to compute.\n",
    "  - An extremely large k approaches leave-one-out CV (LOOCV), reducing bias but increasing variance between folds and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from tabulate import tabulate\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(kfold.split(X_train_pca, y_train_smote),1):\n",
    "    \n",
    "    log_model.fit(X_train_pca[train_idx], y_train_smote[train_idx])\n",
    "    y_prediction = log_model.predict(X_train_pca[test_idx])\n",
    "    print(f\"Fold {fold} Classification report \")\n",
    "    print(tabulate(pd.DataFrame(classification_report(y_train_smote[test_idx], y_prediction, output_dict=True)).transpose(), headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    print(\"*******************************************************\")\n",
    "    ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_train_smote[test_idx], y_prediction), display_labels=[\"Non-Default\", \"Default\"]).plot(cmap=\"Blues\", values_format=\".2f\")\n",
    "\n",
    "    plt.figure(figsize = (10,8))\n",
    "    fpr, tpr, threshold = roc_curve(y_train_smote[test_idx], y_prediction)\n",
    "    roc_score = roc_auc_score(y_train_smote[test_idx], y_prediction)\n",
    "    plt.plot(fpr, tpr, label = f\"Log Reg with SMOTE ROC Score:{roc_score}\")\n",
    "    plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "    plt.title(f\"AUC Curve for Fold {fold} Test Log Reg Classifier\")\n",
    "    plt.xlabel(\"FPR\")\n",
    "    plt.ylabel(\"TPR\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>**Storing Validation Splits**</u>\n",
    "- This code segment stores the validation folds so that we can regenerate the ROC curve. The current train_test_split appears to have been poorly chosen, which likely caused the low metric scores. In a business context, this approach is used to replicate a validation split in order to present a realistic performance representation to stakeholders. It is important to show this process in code to demonstrate that we are not cherry-picking the most favorable split, but that the model performs well in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_results = []\n",
    "for fold_index, (train_idx, test_idx) in enumerate(kfold.split(X_train_pca, y_train_smote), 1):\n",
    "    X_train, X_test = X_train_pca[train_idx], X_train_pca[test_idx]\n",
    "    y_train, y_test = y_train_smote.iloc[train_idx], y_train_smote.iloc[test_idx]\n",
    "    model = LogisticRegression(random_state = 32)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_prob = model.predict_proba(X_test)[:,1] #Predict Probability of Class 1\n",
    "    score = roc_auc_score(y_test, y_prob)\n",
    "    fold_results.append({\"Fold\": fold_index, \"Train_idx\": list(train_idx), \"Test_idx\": list(test_idx), \"ROC Score\": score})\n",
    "\n",
    "cv_fold_df = pd.DataFrame(fold_results).reset_index(drop=True)\n",
    "print(cv_fold_df.head(10))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find Optimal Fold to graph ROC ---> I have the Optimal FOld but Idk how to apply it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <u> **Model Selection Variables - RFE and Backward Selection** </u>\n",
    "\n",
    "- Do not simply remove features—consider their contextual importance.\n",
    "\n",
    "- Model selection must be approached with care. Typically, feature selection is performed before PCA, but we include it here to demonstrate that it is also a viable approach to improving model performance. However, since PCA has already been applied, removing a feature can distort the transformation. For example, if PCA is set with n_components=0.95 (to retain 95% of variance), removing even a single component can reduce the total explained variance in the model. Therefore, feature selection after PCA must be applied cautiously.\n",
    "\n",
    "- There are two common model selection techniques:\n",
    "    1. **Sequential Feature Selection (SFS)**:<br>\n",
    "     - This is a business-standard method that can be performed as forward or backward selection. <br>\n",
    "     \n",
    "        A) **Forward selection**: starts with zero features and iteratively adds the feature that improves the model most. <br>\n",
    "        \n",
    "        B) **Backward selection**: starts with all features and removes the feature with the weakest predictive power at each step. <br>\n",
    "\n",
    "        Since this is a classification problem using Logistic Regression, the default scoring metric is accuracy, which can be customized with scoring=\"evaluation_metric\".\n",
    "\n",
    "    2. **Recursive Feature Elimination (RFE)**:\n",
    "        - RFE begins with all features and removes them iteratively based on the estimator’s feature importance. <br>\n",
    "        \n",
    "        - For **Logistic Regression**, feature importance comes from **coef_** (the weight of each feature). RFE ranks features by the absolute value of the coefficients (|coef|) and removes the least significant feature in each iteration. <br>\n",
    "\n",
    "        - For **tree-based models**, RFE uses **feature_importances_**, which by default relies on Gini impurity reduction. The feature contributing the least to reducing Gini impurity is removed first. <br>\n",
    "\n",
    "- **Important note**: Even if a feature is deemed “least significant” by RFE or SFS, it does not necessarily mean the feature is useless. It may have important interactions with other features or provide critical contextual insights:\n",
    "    - For example, in predicting credit default, removing has_house without considering its context could be misleading, as owning a house may indicate a lower likelihood of default.\n",
    "\n",
    "- When features interact or are transformed to capture more information (e.g., combining age and income into income_per_age), this is called feature engineering. Such engineered features can be more predictive than individual raw features. This is why we dont remove a feature immediately and have to critically think first\n",
    "\n",
    "- **PCA and interactions**: Since PCA transforms features into components, direct removal of features can affect the principal components and reduce explained variance. Interactions between features can also be lost if original features are removed prior to transformation. Careful consideration of feature importance in context is therefore crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE, SequentialFeatureSelector\n",
    "sfs = SequentialFeatureSelector(estimator = log_model, n_features_to_select= 80, direction=\"backward\") #Backward Feature Selection on Log Regression Model\n",
    "print(\"Parameters of Backward Feature Selection Algorithm: \", sfs.get_params())\n",
    "sfs.fit(X_train, y_train)\n",
    "selected_features = sfs.get_support()\n",
    "not_selected = (X.columns[selected_features == False])\n",
    "# print(\"Features Selected: \", X.columns[selected_features])\n",
    "print(\"Features Removed: \", not_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator=log_model, n_features_to_select=104, step=1) #Gini Criterion\n",
    "rfe.fit(X_train_smote, y_train_smote)\n",
    "features_selected = X.columns[rfe.get_support()]\n",
    "print(\"Parameters of Recursive Feature Elimination: \", rfe.get_params())\n",
    "print(\"Features Selected: \" ,rfe.get_support())\n",
    "print(\"Features not Selected: \" , X.columns[rfe.get_support() == False])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **<u>Improving Baseline Model RF</u>**\n",
    "\n",
    "- Random Forest is Forest Model so its more robust to high correlation but looking at feature_importance of the rf tree is telling us that its unable to find any important features that can help us predict for the TARGET feature thus as a result we should not utilize this model in the final model selection and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#High Chance PC is removing importnat information, RF is tree it can handle feature scaling and correlated features(I dont think its good)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42, class_weight=\"balanced\") #balanced_subsample\n",
    "rf_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "y_pred_val = rf_model.predict(X_val)\n",
    "y_pred_train = rf_model.predict(X_train_smote)\n",
    "y_pred_test = rf_model.predict(X_test)\n",
    "\n",
    "print(\"RF Accuracy Train Score:\", accuracy_score(y_train_smote, y_pred_train))\n",
    "print(\"RF Precision Train Score: \", precision_score(y_train_smote, y_pred_train))\n",
    "print(\"RF Recall Train Score: \", recall_score(y_train_smote, y_pred_train))\n",
    "print(\"RF F1 Train Score: \", f1_score(y_train_smote, y_pred_train))\n",
    "\n",
    "print(\"RF Accuracy Validation Score:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"RF Precision Validation Score: \", precision_score(y_val, y_pred_val))\n",
    "print(\"RF Recall Validation Score: \", recall_score(y_val, y_pred_val))\n",
    "print(\"RF F1 Validation Score: \", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"RF Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"RF Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"RF Recall Test Score: \", recall_score(y_test, y_pred_test)) #Super low means that is overfitting\n",
    "print(\"RF F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train and Test and Validation]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train_smote, y_pred_train)\n",
    "roc_score = roc_auc_score(y_train_smote, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = f\"RF Classifier Train with ROC: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of RF\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr, tpr, color=\"red\", label = f\"ROC CURVE for RF Validation with {roc_score}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", label=f\"ROC CURVE Random model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"RF ROC CURVE for Validation vs Random Model\")\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = f\"RF Classifier Test with ROC Score: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of RF\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Feature Importance in RF Tree(Yea so like RF just sucks it cant find any important features all it sees is noise) --> Option is to like do Feature Selection it could improve but we will just not use this\n",
    "print(pd.Series(rf_model.feature_importances_, index = X_train.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Improving XGBoost Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#High Chance PC is removing importnat information, RF is tree it can handle feature scaling and correlated features(I dont think its good)\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, auc, precision_score, accuracy_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "xgb_model1 = xgb_model\n",
    "xgb_model.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "\n",
    "y_pred_val = xgb_model.predict(X_val)\n",
    "y_pred_train = xgb_model.predict(X_train_smote)\n",
    "y_pred_test = xgb_model.predict(X_test)\n",
    "\n",
    "print(\"XGB Accuracy Train Score:\", accuracy_score(y_train_smote, y_pred_train))\n",
    "print(\"XGB Precision Train Score: \", precision_score(y_train_smote, y_pred_train))\n",
    "print(\"XGB Recall Train Score: \", recall_score(y_train_smote, y_pred_train))\n",
    "print(\"XGB F1 Train Score: \", f1_score(y_train_smote, y_pred_train))\n",
    "\n",
    "print(\"XGB Accuracy Validation Score:\", accuracy_score(y_val, y_pred_val))\n",
    "print(\"XGB Precision Validation Score: \", precision_score(y_val, y_pred_val))\n",
    "print(\"XGB Recall Validation Score: \", recall_score(y_val, y_pred_val))\n",
    "print(\"XGB F1 Validation Score: \", f1_score(y_val, y_pred_val))\n",
    "\n",
    "\n",
    "print(\"XGB Accuracy Test Score: \", accuracy_score(y_test, y_pred_test))\n",
    "print(\"XGB Precision Test Score: \", precision_score(y_test, y_pred_test))\n",
    "print(\"XGB Recall Test Score: \", recall_score(y_test, y_pred_test)) #Super low means that is overfitting\n",
    "print(\"XGB F1 Test Score: \", f1_score(y_test, y_pred_test))\n",
    "\n",
    "#Create a ROC Curve[For Train and Test and Validation]\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_train_smote, y_pred_train)\n",
    "roc_score = roc_auc_score(y_train_smote, y_pred_train)\n",
    "plt.plot(fpr, tpr, label = f\"XGB Classifier Train with ROC: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Train of XGB\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fpr, tpr, threshold = roc_curve(y_val, y_pred_val)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.plot(fpr, tpr, color=\"red\", label = f\"ROC CURVE for XGB Validation with {roc_score}\")\n",
    "plt.plot([0,1], [0,1], linestyle=\"--\", label=f\"ROC CURVE Random model\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.title(\"XGB ROC CURVE for Validation vs Random Model\")\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "fpr, tpr, threshold = roc_curve(y_test, y_pred_test)\n",
    "roc_score = roc_auc_score(y_test, y_pred_test)\n",
    "plt.plot(fpr, tpr, label = f\"XGB Classifier Test with ROC Score: {roc_score}\")\n",
    "plt.plot([0,1],[0,1],linestyle=\"--\",label=\"Random Model\")\n",
    "plt.title(\"AUC Curve for Test of XGB\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Feature Importance in RF Tree(Yea so like RF just sucks it cant find any important features all it sees is noise) --> Option is to like do Feature Selection it could improve but we will just not use this\n",
    "print(pd.Series(xgb_model.feature_importances_, index = X_train.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
