{"cells":[{"cell_type":"markdown","id":"46fe9303","metadata":{},"source":["## Libraries and Packages Required to run project"]},{"cell_type":"code","execution_count":null,"id":"595059dc-7204-4f5b-a8e7-b7422791b080","metadata":{"id":"595059dc-7204-4f5b-a8e7-b7422791b080","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["%pip install seaborn\n","%pip install scikit-learn\n","%pip install statsmodels\n","%pip install tabulate\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import sklearn\n","import math\n","import tabulate\n","import seaborn as sns\n","from tabulate import tabulate\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import LogisticRegression\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","\n","pd.set_option(\"display.max_rows\", None)\n","pd.set_option(\"display.max_columns\", None)"]},{"cell_type":"markdown","id":"08159a29","metadata":{},"source":["## Information regarding shape, features of data"]},{"cell_type":"code","execution_count":2,"id":"43a38f2c","metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1760471764493,"user":{"displayName":"Michael Liu","userId":"01475140158270491072"},"user_tz":240},"id":"43a38f2c","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["'''\n","Returns information regarding the dataframe and also if it has missing values and the descriptive stats for both Categorical and Numerical variables\n","\n","Parameter:\n","    -df[DataFrame]: Contains the data that we want to look into more information about\n","\n","Returns:\n","    - Nothing treat as a void function that just used for printing info\n","'''\n","def information(df):\n","    pd.set_option(\"display.max_columns\", None)\n","    pd.set_option(\"display.max_rows\", None)\n","\n","\n","    print(\"Column names:\")\n","    print(f\"Feature Names: {df.columns}\\n\")\n","    print(f\"Number of Features (Columns): {df.shape[1]}\\n\")\n","    print(f\"Number of Observations (Rows): {df.shape[0]}\\n\")\n","    print(\"DataFrame Info:\")\n","    df.info()\n","    print(\"\\n\")\n","\n","    #Majority of the features in our dataset have missing values\n","    print(\"Columns with Missing Values (True = has NaNs):\")\n","    print(f\"{df.isna().any()}\\n\")\n","\n","    print(\"Descriptive Statistics:\\n\")\n","    print(f\"{df.describe(include='all')}\")\n","    print(\"******************************\\n\")\n","    print(\"First 10 Observations of dataset:\\n\")\n","    print(df.head(10))\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","id":"4ffbf86e","metadata":{},"source":["## Train Dataset(The Dataset the model will be training on)"]},{"cell_type":"code","execution_count":5,"id":"82c719dc","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"elapsed":181,"status":"error","timestamp":1760471784556,"user":{"displayName":"Michael Liu","userId":"01475140158270491072"},"user_tz":240},"id":"82c719dc","outputId":"aba5d48d-9179-47c2-deea-c4ae0fef859c","vscode":{"languageId":"shellscript"}},"outputs":[{"ename":"NameError","evalue":"name 'information' is not defined","output_type":"error","traceback":["\u001b[31m---------------------------------------------------------------------------\u001b[39m","\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)","\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Training Data Information -- Automatically does Data Integration on other datasets\u001b[39;00m\n\u001b[32m      2\u001b[39m df_train = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33m/Applications/CISC251/home-credit-default-risk (1)/application_train.csv\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43minformation\u001b[49m(df_train)\n\u001b[32m      4\u001b[39m missing_vals_train = df_train.columns[df_train.isna().any()].tolist()\n","\u001b[31mNameError\u001b[39m: name 'information' is not defined"]}],"source":["#Training Data Information -- Automatically does Data Integration on other datasets\n","df_train = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/application_train.csv\")\n","information(df_train)\n","missing_vals_train = df_train.columns[df_train.isna().any()].tolist()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"id":"e47d669b","metadata":{"id":"e47d669b","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Total Unique Values for each Feature in Train Dataset -- Tells us if Numerical is Discrete or Continuous\n","total_count_unique = []\n","df = pd.DataFrame()\n","for col in df_train.columns.tolist():\n","    total_count_unique.append(df_train[col].nunique())\n","df = pd.DataFrame({\"Feature Name\": df_train.columns.tolist(), \"Total Unique Value\": total_count_unique})\n","print(f\"Data Type of Each Feature\")\n","print(df_train.dtypes)\n","print(\"**************************************************************\")\n","print(f\"Dataframe consisting of Unique Values of Each Feature\")\n","print(tabulate(df, headers=\"keys\"))"]},{"cell_type":"markdown","id":"6d2f8152","metadata":{},"source":["## Test Dataset"]},{"cell_type":"code","execution_count":null,"id":"7be305fd","metadata":{"id":"7be305fd","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Testing Data Set\n","df_test = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/application_test.csv\")\n","information(df_test)\n","missing_vals_train = df_train.columns[df_train.isna().any()].tolist()\n","\n","\n","'''\n","df_test has one less feature than df_train which is y(\"TARGET\")\n","df_test does not have a True Value or a True label as our model should be predicting the hypothesis\n","df_test has less observations: 48744 but this makes sense as Train should have 60-70% data while Test should contain 40-30\n","Columns with missing values are contained in the list missing_vals_test\n","'''"]},{"cell_type":"code","execution_count":null,"id":"27e77566","metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Total Unique Values for each Feature in Test Dataset -- Tells us if Numerical is Discrete or Continuous\n","total_count_unique = []\n","df = pd.DataFrame()\n","for col in df_test.columns.tolist():\n","    total_count_unique.append(df_test[col].nunique())\n","df = pd.DataFrame({\"Feature Name\": df_test.columns.tolist(), \"Total Unique Value\": total_count_unique})\n","print(f\"Data Type of Each Feature\")\n","print(df_test.dtypes)\n","print(\"**************************************************************\")\n","print(f\"Dataframe consisting of Unique Values of Each Feature\")\n","print(tabulate(df, headers=\"keys\"))"]},{"cell_type":"code","execution_count":null,"id":"0b48e55c","metadata":{"id":"0b48e55c","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Other Datasets that we might try to practice Data Integration(Read this if we want more information)\n","df_bureaubal = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/bureau_balance.csv\")\n","df_bureau = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/bureau.csv\")\n","df_ccbal = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/credit_card_balance.csv\")\n","df_POSCASH = pd.read_csv(\"/df_trainlications/CISC251/home-credit-default-risk (1)/POS_CASH_balance.csv\")\n","df_installment = pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/installments_payments.csv\")\n","df_prev= pd.read_csv(\"/Applications/CISC251/home-credit-default-risk (1)/previous_application.csv\")\n"]},{"cell_type":"code","execution_count":null,"id":"d8e54fdc","metadata":{"id":"d8e54fdc","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Our Predictors Variables and Target Variable for Classification Problem\n","predictors = df_train.drop(columns=\"TARGET\").columns.tolist()\n","target = \"TARGET\""]},{"cell_type":"markdown","id":"362a5054","metadata":{"id":"362a5054"},"source":["## Exploratory Data Analysis(EDA) --> Looking At Information(Temporary Changes):\n","\n","1) Examine relationship between features(Correlation, Identify if Categorical or Numerical) [Basically Done]\n","2) Create Charts for Descriptive Statistics\n","3) Describe shape of data [] (Also done just df.shape to get total obs and total feature)\n","4) Detect outliers and missing values\n"]},{"cell_type":"markdown","id":"23bf3f61","metadata":{},"source":["Rough Sorting each Feature to Continuous/Discrete Numerical or Categorical"]},{"cell_type":"code","execution_count":16,"id":"19b70996","metadata":{"id":"19b70996","vscode":{"languageId":"shellscript"}},"outputs":[{"name":"stdout","output_type":"stream","text":["SK_ID_CURR is Continuous Numerical Variable\n","\n","TARGET is Discrete Numerical Variable\n","\n","NAME_CONTRACT_TYPE is Categorical Variable\n","\n","CODE_GENDER is Categorical Variable\n","\n","FLAG_OWN_CAR is Categorical Variable\n","\n","FLAG_OWN_REALTY is Categorical Variable\n","\n","CNT_CHILDREN is Discrete Numerical Variable\n","\n","AMT_INCOME_TOTAL is Continuous Numerical Variable\n","\n","AMT_CREDIT is Continuous Numerical Variable\n","\n","AMT_ANNUITY is Continuous Numerical Variable\n","\n","AMT_GOODS_PRICE is Continuous Numerical Variable\n","\n","NAME_TYPE_SUITE is Categorical Variable\n","\n","NAME_INCOME_TYPE is Categorical Variable\n","\n","NAME_EDUCATION_TYPE is Categorical Variable\n","\n","NAME_FAMILY_STATUS is Categorical Variable\n","\n","NAME_HOUSING_TYPE is Categorical Variable\n","\n","REGION_POPULATION_RELATIVE is Continuous Numerical Variable\n","\n","DAYS_BIRTH is Continuous Numerical Variable\n","\n","DAYS_EMPLOYED is Continuous Numerical Variable\n","\n","DAYS_REGISTRATION is Continuous Numerical Variable\n","\n","DAYS_ID_PUBLISH is Continuous Numerical Variable\n","\n","OWN_CAR_AGE is Continuous Numerical Variable\n","\n","FLAG_MOBIL is Discrete Numerical Variable\n","\n","FLAG_EMP_PHONE is Discrete Numerical Variable\n","\n","FLAG_WORK_PHONE is Discrete Numerical Variable\n","\n","FLAG_CONT_MOBILE is Discrete Numerical Variable\n","\n","FLAG_PHONE is Discrete Numerical Variable\n","\n","FLAG_EMAIL is Discrete Numerical Variable\n","\n","OCCUPATION_TYPE is Categorical Variable\n","\n","CNT_FAM_MEMBERS is Discrete Numerical Variable\n","\n","REGION_RATING_CLIENT is Discrete Numerical Variable\n","\n","REGION_RATING_CLIENT_W_CITY is Discrete Numerical Variable\n","\n","WEEKDAY_APPR_PROCESS_START is Categorical Variable\n","\n","HOUR_APPR_PROCESS_START is Continuous Numerical Variable\n","\n","REG_REGION_NOT_LIVE_REGION is Discrete Numerical Variable\n","\n","REG_REGION_NOT_WORK_REGION is Discrete Numerical Variable\n","\n","LIVE_REGION_NOT_WORK_REGION is Discrete Numerical Variable\n","\n","REG_CITY_NOT_LIVE_CITY is Discrete Numerical Variable\n","\n","REG_CITY_NOT_WORK_CITY is Discrete Numerical Variable\n","\n","LIVE_CITY_NOT_WORK_CITY is Discrete Numerical Variable\n","\n","ORGANIZATION_TYPE is Categorical Variable\n","\n","EXT_SOURCE_1 is Continuous Numerical Variable\n","\n","EXT_SOURCE_2 is Continuous Numerical Variable\n","\n","EXT_SOURCE_3 is Continuous Numerical Variable\n","\n","APARTMENTS_AVG is Continuous Numerical Variable\n","\n","BASEMENTAREA_AVG is Continuous Numerical Variable\n","\n","YEARS_BEGINEXPLUATATION_AVG is Continuous Numerical Variable\n","\n","YEARS_BUILD_AVG is Continuous Numerical Variable\n","\n","COMMONAREA_AVG is Continuous Numerical Variable\n","\n","ELEVATORS_AVG is Continuous Numerical Variable\n","\n","ENTRANCES_AVG is Continuous Numerical Variable\n","\n","FLOORSMAX_AVG is Continuous Numerical Variable\n","\n","FLOORSMIN_AVG is Continuous Numerical Variable\n","\n","LANDAREA_AVG is Continuous Numerical Variable\n","\n","LIVINGAPARTMENTS_AVG is Continuous Numerical Variable\n","\n","LIVINGAREA_AVG is Continuous Numerical Variable\n","\n","NONLIVINGAPARTMENTS_AVG is Continuous Numerical Variable\n","\n","NONLIVINGAREA_AVG is Continuous Numerical Variable\n","\n","APARTMENTS_MODE is Continuous Numerical Variable\n","\n","BASEMENTAREA_MODE is Continuous Numerical Variable\n","\n","YEARS_BEGINEXPLUATATION_MODE is Continuous Numerical Variable\n","\n","YEARS_BUILD_MODE is Continuous Numerical Variable\n","\n","COMMONAREA_MODE is Continuous Numerical Variable\n","\n","ELEVATORS_MODE is Continuous Numerical Variable\n","\n","ENTRANCES_MODE is Continuous Numerical Variable\n","\n","FLOORSMAX_MODE is Continuous Numerical Variable\n","\n","FLOORSMIN_MODE is Continuous Numerical Variable\n","\n","LANDAREA_MODE is Continuous Numerical Variable\n","\n","LIVINGAPARTMENTS_MODE is Continuous Numerical Variable\n","\n","LIVINGAREA_MODE is Continuous Numerical Variable\n","\n","NONLIVINGAPARTMENTS_MODE is Continuous Numerical Variable\n","\n","NONLIVINGAREA_MODE is Continuous Numerical Variable\n","\n","APARTMENTS_MEDI is Continuous Numerical Variable\n","\n","BASEMENTAREA_MEDI is Continuous Numerical Variable\n","\n","YEARS_BEGINEXPLUATATION_MEDI is Continuous Numerical Variable\n","\n","YEARS_BUILD_MEDI is Continuous Numerical Variable\n","\n","COMMONAREA_MEDI is Continuous Numerical Variable\n","\n","ELEVATORS_MEDI is Continuous Numerical Variable\n","\n","ENTRANCES_MEDI is Continuous Numerical Variable\n","\n","FLOORSMAX_MEDI is Continuous Numerical Variable\n","\n","FLOORSMIN_MEDI is Continuous Numerical Variable\n","\n","LANDAREA_MEDI is Continuous Numerical Variable\n","\n","LIVINGAPARTMENTS_MEDI is Continuous Numerical Variable\n","\n","LIVINGAREA_MEDI is Continuous Numerical Variable\n","\n","NONLIVINGAPARTMENTS_MEDI is Continuous Numerical Variable\n","\n","NONLIVINGAREA_MEDI is Continuous Numerical Variable\n","\n","FONDKAPREMONT_MODE is Categorical Variable\n","\n","HOUSETYPE_MODE is Categorical Variable\n","\n","TOTALAREA_MODE is Continuous Numerical Variable\n","\n","WALLSMATERIAL_MODE is Categorical Variable\n","\n","EMERGENCYSTATE_MODE is Categorical Variable\n","\n","OBS_30_CNT_SOCIAL_CIRCLE is Continuous Numerical Variable\n","\n","DEF_30_CNT_SOCIAL_CIRCLE is Discrete Numerical Variable\n","\n","OBS_60_CNT_SOCIAL_CIRCLE is Continuous Numerical Variable\n","\n","DEF_60_CNT_SOCIAL_CIRCLE is Discrete Numerical Variable\n","\n","DAYS_LAST_PHONE_CHANGE is Continuous Numerical Variable\n","\n","FLAG_DOCUMENT_2 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_3 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_4 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_5 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_6 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_7 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_8 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_9 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_10 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_11 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_12 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_13 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_14 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_15 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_16 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_17 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_18 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_19 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_20 is Discrete Numerical Variable\n","\n","FLAG_DOCUMENT_21 is Discrete Numerical Variable\n","\n","AMT_REQ_CREDIT_BUREAU_HOUR is Discrete Numerical Variable\n","\n","AMT_REQ_CREDIT_BUREAU_DAY is Discrete Numerical Variable\n","\n","AMT_REQ_CREDIT_BUREAU_WEEK is Discrete Numerical Variable\n","\n","AMT_REQ_CREDIT_BUREAU_MON is Continuous Numerical Variable\n","\n","AMT_REQ_CREDIT_BUREAU_QRT is Discrete Numerical Variable\n","\n","AMT_REQ_CREDIT_BUREAU_YEAR is Continuous Numerical Variable\n","\n"]}],"source":["  #Categorizes Roughly Variable Names without taking into account Semantics and each feature definition\n","  for x in df_train.columns:\n","    unique = len(df_train[x].unique())\n","    type = df_train[x].dtype\n","    if(unique >= 20 and type != object):\n","        print(f\"{x} is Continuous Numerical Variable\\n\")\n","\n","    elif(unique < 20 and type != object):\n","        print(f\"{x} is Discrete Numerical Variable\\n\")\n","    else:\n","        print(f\"{x} is Categorical Variable\\n\")"]},{"cell_type":"markdown","id":"d9123d65","metadata":{},"source":["## Temporary Encoding to see Correlation Information"]},{"cell_type":"code","execution_count":null,"id":"6ccbb36d","metadata":{"id":"6ccbb36d","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["'''Categorical Features Label Encoding to convert from Categorical to Numerical'''\n","encoded_features = []\n","df_encoded = df_train.copy()\n","for column in df_encoded.select_dtypes(include=\"object\"):\n","    print(f\"Encoded Column: {column}\")\n","    encoded_features.append(column)\n","    label = LabelEncoder()\n","    df_encoded[f\"{column}_encoded\"] = label.fit_transform(df_encoded[column])"]},{"cell_type":"code","execution_count":null,"id":"d3330224","metadata":{"id":"d3330224","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Also tells us if linear relationship with TARGET - df.corr() = 0 means No linear relationship and df.corr() -> +/-1 strong correlation in positive or negative\n","\n","df_encoded.drop(columns=encoded_features, inplace=True,errors=\"ignore\")\n","correlation_matrix = pd.DataFrame(df_encoded.corr(method=\"spearman\")[\"TARGET\"].sort_values(ascending=False)) \n","\n","'''\n","Create a Pearson Coefficent Test to see if Feature has a Positive or Negative Correlation with Target\n","\n","Parameters:\n","    - df[DataFrame]: Contains the data that is to be analyzed\n","\n","Returns:\n","    - new_df[DataFrame]: Contains the DataFrame that contains whether or not a feature has a positive or negative correlation with target\n","'''\n","def pearson_test(df):\n","    new_df = df.copy()\n","    correlation_sign = []\n","    for val in df.values.astype(float):\n","            if val < 0:\n","                correlation_sign.append(\"Negative Correlation\")\n","            else:\n","                correlation_sign.append(\"Positive Correlation\")\n","    new_df[\"Correlation Sign\"] = correlation_sign\n","    return new_df\n","\n","correlation_matrix = pearson_test(correlation_matrix)\n","print(tabulate(correlation_matrix, headers=\"keys\"))\n","\n","threshold = 0.05\n","high_corr = high_corr[(abs(high_corr) > threshold)].dropna(axis = 0, how=\"all\").dropna(axis=1, how=\"all\")\n","plt.figure(figsize=(40,20))\n","sns.heatmap(high_corr, annot=True,cmap=\"coolwarm\", fmt=\".2f\", linewidth=.5, center=0)\n","plt.title(\"Correlation Matrix [NonLinear] of Train Dataset with Correlation with Target greater than 0.05\")\n","plt.show()\n","plt.close()"]},{"cell_type":"code","execution_count":null,"id":"9c633b1c","metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[],"source":["# Compute correlations with TARGET\n","corrs = df_encoded.corr(numeric_only=True)['TARGET'].drop('TARGET').sort_values()\n","\n","# Convert to DataFrame for plotting\n","corr_df = corrs.reset_index()\n","corr_df.columns = ['Feature', 'Correlation']\n","\n","# Show only strongest correlations (absolute value > 0.02)\n","strong_corr = corr_df[abs(corr_df['Correlation']) > 0.05]\n","plt.figure(figsize=(8, len(strong_corr)/3))\n","sns.barplot(data=strong_corr, y='Feature', x='Correlation',\n","            hue='Correlation', palette='coolwarm', legend=False)\n","plt.title('Strongest Correlations with TARGET')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"c5604414","metadata":{},"source":["Running Correlation Matrix we get correlation of each feature linearly if we run method with pearson and this will give us a dataframe that holds the result. We ran it with pearson and noticed that a lot of the features were not correlated with the target and the heat map did not show many multicolinear features. With this we decided to run the Spearman method and test for non linear relationships and noticed much more multcolinear features as the correlation between two was high. The threshold to test if multicolinear was assigned by default to be 0.7\n","\n","1) REGION_RATING_WITH_CLIENT with REGION_RATING_WITH_CLIENT_CITY had a correlation 0.95\n","2) DAYS_EMPLOYED with FLAG_EMP_PHONE had correlation of -1.00\n","3) FLOORSMAX_AVG with FLOORSMAX_MODE FLOORSMAX_MEDI had correlation of 0.9+\n","4) WALLSTYPE_MODE_ENCODED with HOUSE_TYPE_MODE_ENCODED and EMERGENCY_MODE_ENCODED had 0.8+ \n","5) ELEVATOR_AVG and ELEVATOR_MEDI had a 0.9+ \n","\n","It is best to remove one of these highly correlated features but this must be done based on context and significance with other relationships"]},{"cell_type":"code","execution_count":null,"id":"24268344","metadata":{"id":"24268344","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["'''\n","Checks on Multicolinearity between each feature pair in our train dataset as Multicolinearity can affect model performance\n","\n","Paramaters:\n","    - df[DataFrame]: Contains the feature(s) and observation of our data\n","    - method[String]: Contains the method that you want to do Correlation by(Non linear takes longer)\n","Returns:\n","    - final_df[DataFrame]: Contains the feature pair and correlation score between 2 features -- All values that have a correlation score above threshold(T) of 0.8\n","'''\n","def multicolinearity(df,method):\n","    multicolinearity_features = {}\n","    list_of_features = df.columns.tolist()\n","    for i in range(0, df.shape[1]):\n","        #Can ignore Colinearity between itself and (A,B) corr same as (B,A) which is why we have i + 1 as start index\n","        for j in range(i+1, df.shape[1]):\n","            correlation_score = df[list_of_features[i]].corr(df[list_of_features[j]], method = method)\n","            if abs(correlation_score) >= 0.8:\n","                multicolinearity_features.update({(list_of_features[i], list_of_features[j]): correlation_score})\n","    final_df = pd.DataFrame([(k,v) for k,v in multicolinearity_features.items()], columns=[\"Feature Pair\", \"Correlation Score\"])\n","\n","    return final_df\n","\n","#Already calculated correlation score with TARGET column so like we dont need to check multicolinearity (>= 0.7 chance its Multicolinear)\n","# features_mc = multicolinearity(df_encoded.drop(columns=[\"TARGET\"]), method=\"spearman\")\n","# print(tabulate(features_mc, headers=\"keys\"))\n"]},{"cell_type":"markdown","id":"ba0ba14d","metadata":{},"source":["More ways to remove Multicolinear related features, it becomes easier to see what is causing the changes in the dependent variable. Multicolinearity can lead to an increase in variance of a model which ultimately leads to overfitting. As a result if the model is overfitting than its not really learning the patterns in data but it will fit very well with the train dataset. When it comes to new datasets it will be unable to do well on unseen data such as the test dataset as it has not properly learned of the patterns"]},{"cell_type":"code","execution_count":null,"id":"4db8d6c7","metadata":{"id":"4db8d6c7","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Create a VIF Function - For Multicolinearity[>5 --> Multicolinear  ]\n","'''\n","Calculates the Variance Inflation Factor - If we have a value >= 5 then multicolinearity exists and we can remove a feature using our multicolinear function\n","\n","Parameters:\n","    - df[DataFrame]: Consists all our data including number of features and number of observations\n","Return:\n","    - df2[DataFrame]: Consists of all the Features and their corresponding observations\n","    - dict1[Dictionary]: Consists of The Features that have a VIF >= 5\n","'''\n","def VIF(df):\n","    dict = {}\n","    list_of_features = df.columns.tolist()\n","    input_features = df[list_of_features].dropna()\n","    VIF_df = pd.DataFrame()\n","    VIF_df[\"Features\"] = df.columns\n","    VIF_df[\"VIF SCORE\"] = [variance_inflation_factor(input_features.values, i) for i in range(len(input_features.columns))]\n","    for i in range(0,VIF_df.shape[0]):\n","        # print(VIF_df.iloc[i,1]) #Ith row of VIF_SCORE\n","        if VIF_df.iloc[i,1] >= 5.00:\n","             dict.update({VIF_df[\"Features\"].iloc[i]: VIF_df.iloc[i, 1]})\n","\n","    return (VIF_df, dict)\n","\n","df2, dict1 = VIF(df_encoded)\n","\n","print(df2, dict1)"]},{"cell_type":"code","execution_count":null,"id":"a6f4f8b8","metadata":{"id":"a6f4f8b8","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Check for Linear Relationship between A feature and target feature -- Too many plots use Density or hist(Use a random sample of df_train)\n","#Test on 2 high correlation variables like DEF_30_CNT_SOCIAL_CIRCLE and DEF_60_CNT_SOCIAL_CIRCLE\n","#Using fill as substitute will fix later and remove\n","'''\n","Finds if there a linear relationship between Feat A and Feat B(Draws a scatter and prints R and R^2)\n","\n","Parameters:\n","    df[DataFrame]: Contains all Features and Observation in data\n","    A[String]: Name of Feature A\n","    B[String]: Name of Feature B\n","\n","Returns:\n","    - Nothing it shows a graphical representation if its a linear relationship and also prints R and R^2 value\n","'''\n","def linear_relationship(df, A, B):\n","\n","    plt.scatter(df[A], df[B], label = \"Train_pair\", s=2)\n","    #Line of best fit(m = slope, b = intercept) --> if line is flat means no correlation or relationship between A and B is super weak\n","    m,b = np.polyfit(df[A].fillna(df[A].mean()),df[B].fillna(df[B].mean()),1)\n","    line = m * df[A].fillna(df[A].mean()) + b\n","    plt.plot(df[A].fillna(df[A].mean()),line,color=\"red\", linewidth=0.5, label=f\"{m:.2f}x + {b:.2f}\")\n","    plt.title(f\"Checking Linear Relationship between Feature {A} and Feature {B}\")\n","    plt.xlabel(f\"Feature: {A}\")\n","    plt.ylabel(f\"Feature: {B}\")\n","    plt.xlim(df[A].min(), df[A].max())\n","    plt.ylim(df[B].min(), df[B].max())\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()\n","    plt.close()\n","\n","    #Low Scores means not linearly related\n","    print(f\"Correlation(R) between {A} and {B} : {df[A].corr(df[B])}\\n\")\n","    print(f\"R^2[Proportion of {A} explained by {B}: {(df[A].corr(df[B]))**2}\") #could just r2_score\n","\n","\n","#Check between 2 features if they are linearly related\n","linear_relationship(df_encoded.sample(frac=1.0), \"FLOORSMAX_AVG\", \"FLOORSMAX_MEDI\") #\n","\n"]},{"cell_type":"code","execution_count":null,"id":"944fb51c","metadata":{"id":"944fb51c","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["\n","'''\n","Information regarding missing values for each input feature in df. Used to see whether a univariate or multivariate imputation should be performed \n","depending on amount of missing vals\n","\n","Paramaters:\n","    - df[DataFrame]: Contains the data\n","Returns:\n","    - Nothing, it can be considered a void function that has a bunch of print statement regarding information to do with missing vals\n","'''\n","def missing_val_info(df, variable_name=None):\n","    unique, missing_values, missing_proportion = [], [], []\n","    df_copy = df.copy()\n","    if \"TARGET\" in df_copy.columns:\n","        df_copy.drop(columns=[\"TARGET\"], inplace = True)\n","    for feature in df_copy.columns.str.strip():\n","        missing_values.append(df_copy[feature].isna().sum())\n","        missing_proportion.append((df_copy[feature].isna().sum()/df_copy.shape[0]) * 100)\n","        unique.append(df_copy[feature].nunique())\n","    df_info = pd.DataFrame({\"Feature\": [x for x in df_copy.columns], \"Missing Values\": missing_values, \"Missing Proportion\": missing_proportion,\n","                            \"Unique Values\": unique\n","                          })\n","    print(tabulate(df_info.sort_values(by=\"Missing Proportion\", ascending=False).reset_index(drop=True), headers=\"keys\"))\n"]},{"cell_type":"code","execution_count":null,"id":"3ea4ca42","metadata":{"id":"3ea4ca42","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["#Information about Dataset --> Object already encoded into Numeric\n","print(f\"Total Features in Dataset: {df_encoded.shape[1]}, Total Observation in Dataset: {df_encoded.shape[0]}\")\n","print(\"Total Features in Dataset with at least one missing value:\", len(missing_vals_train))\n","missing_df = pd.DataFrame(missing_vals_train, columns=[\"Feature Name\"])\n","print(tabulate(missing_df, headers=\"keys\"))\n","print(\"********************************\")\n","print(f\"DataFrame Containing Missing Value Info\\n\")\n","missing_val_info(df_encoded)\n","print(\"Inferential Statistics about Features\\n\")\n","print(df_encoded.describe(include=\"all\"))\n","\n"]},{"cell_type":"markdown","id":"7a8a6d1a","metadata":{},"source":["## Identify Relationship between features"]},{"cell_type":"code","execution_count":null,"id":"a2ed01a9","metadata":{"id":"a2ed01a9","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["\n","'''\n","Graph to show the values of TARGET y e [0,1] to see if class imbalance\n","'''\n","print(df_train[\"TARGET\"].value_counts(normalize=True))\n","target_count = df_train[\"TARGET\"].value_counts().sort_index()\n","target_total = target_count.sum()\n","percentage = 100 * (target_count / target_total)\n","\n","\n","fig, ax = plt.subplots(2,1, figsize=(10,8))\n","bar1 = ax[0].bar(target_count.index, target_count.values, color=\"red\")\n","ax[0].bar_label(bar1)\n","ax[0].set_title(\"Distribution of TARGET[Default vs Non Default]\")\n","ax[0].set_xlabel(\"Default vs Non Default\")\n","ax[0].set_ylabel(\"Count\")\n","ax[0].set_xticks([0,1], [\"Non Default(0)\", \"Default(1)\"])\n","\n","bar2 = ax[1].bar(target_count.index, percentage, color=\"Blue\")\n","ax[1].set_ylabel(\"Percentage(%)\", color=\"black\")\n","percent_label = [f\"{p:.2f}%\" for p in percentage]\n","ax[1].bar_label(bar2, label = percent_label, padding = 2)\n","ax[1].set_ylim(0,100)\n","ax[1].set_xticks([0,1], [\"Non Default(0)\", \"Default(1)\"])\n","plt.tight_layout()\n","plt.show()\n","plt.close()\n"]},{"cell_type":"markdown","id":"b96e4257","metadata":{},"source":["The bar graph shows a clear class imbalance in the TARGET feature, where the minority class, Default, is underrepresented. This can lead models like Logistic Regression and KNN to perform poorly, as they treat classes equally and tend to favor the majority class to minimize loss. As a result, the model may rarely predict defaults and learn weak decision boundaries. To address this, imbalance must be handled before training. Metrics like Confusion Matrix and ROC AUC are better suited for evaluating model performance in such cases. Another technique is oversampling the imbalanced class to generate more sample of the class or removing some of the sample for the majority class to cause undersampling. These are risky as there a high chance that important information may be lost in the process so best to use a parameter in the model called class_weight and set to balanced to give minority class more weight. Whats even better use Gradient Descent Model or Ensemble Models "]},{"cell_type":"code","execution_count":null,"id":"1ab34c2d","metadata":{"id":"1ab34c2d","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["'''\n","Histogram for each Feature in Train Dataset but running in batches with each batch consist 15 feature\n","'''\n","batch = 15\n","df_copy = df_train.copy()\n","# features_keep = [i for i in df_copy.columns if df_copy[i].dtype != \"object\"]\n","df_copy = df_copy.select_dtypes(include=[\"number\"])\n","print(f\"Total Numeric Features: {df_copy.shape[1]}\")\n","total_features = df_copy.shape[1]\n","print(df_copy['SK_ID_CURR'].min(), df_copy['SK_ID_CURR'].max())\n","for i in range(0,total_features,batch):\n","\n","    subset = df_copy.iloc[:,i:i+batch]\n","    subset.hist(figsize=(14, 10), sharex=False, sharey=False, edgecolor=\"white\", bins=30)\n","    plt.suptitle(\"Histograms of Numeric Features\", fontsize=16)\n","    plt.tight_layout()\n","    plt.show()\n","\n"]},{"cell_type":"markdown","id":"f5297792","metadata":{},"source":["Histograms are used to understand distribution of numeric features. We can see how data points are spread across values, identify shape of distribution, detect outliers(bars far away from main cluster is outlier), Decide if transformation of feature is needed"]},{"cell_type":"code","execution_count":null,"id":"I9HISHDFFbXV","metadata":{"id":"I9HISHDFFbXV"},"outputs":[],"source":["'''\n","Multivariable visualization: Pairplot of selected features colored by TARGET\n","Uses a sample to keep plotting performant on large data\n","Purpose: Detect Cluster and patterns while also seeing how variables are correlated and distribution of each feature\n","'''\n","features_for_plot = [\n","    \"AMT_CREDIT\",\n","    \"AMT_ANNUITY\",\n","    \"AMT_GOODS_PRICE\",\n","    \"EXT_SOURCE_2\",\n","    \"EXT_SOURCE_1\",\n","    \"EXT_SOURCE_3\",\n","    \"FLAG_OWN_REALTY\",\n","    \"\"\n","]\n","available_features = [c for c in features_for_plot if c in df_encoded.columns] \n","if len(available_features) >= 2:  # need at least 2 variables for a meaningful pairplot\n","    pairplot_df = df_train[[\"TARGET\"] + available_features].dropna()\n","    #Minimuum sample size to give us an overview of the data \n","    sample_size = min(4000, pairplot_df.shape[0]) \n","\n","    #Random Sample of 4000 observations\n","    if sample_size < pairplot_df.shape[0]:\n","        pairplot_df = pairplot_df.sample(n=sample_size, random_state=42)\n","    #Converts Target feature into categorical so that we can \n","    pairplot_df[\"TARGET\"] = pairplot_df[\"TARGET\"].astype(\"category\")\n","  \n","    g = sns.pairplot(\n","        pairplot_df,\n","        hue=\"TARGET\",\n","        corner=True,\n","        diag_kind=\"hist\",\n","        plot_kws={\"alpha\": 0.35, \"s\": 10, \"linewidth\": 0, },\n","        diag_kws={\"bins\": 25, \"alpha\": 0.8, \"color\": \"black\"},\n","    )\n","    g.fig.suptitle(\"Multivariable pairplot of selected features by TARGET\", y=1.02)\n","    plt.show()\n","    plt.close()\n","else:\n","    print(\"Not enough available numerical features for a multivariable pairplot.\")\n","\n"]},{"cell_type":"code","execution_count":null,"id":"5d517af0","metadata":{"id":"5d517af0","vscode":{"languageId":"shellscript"}},"outputs":[],"source":["'''\n","Finding Outliers for Most Significant Variables[How much do Outliers affect our Target Variable for Top 9 most significant features]\n","Get top 9 features by correlation with TARGET\n","Tells us what values for feature using boxplot that are outliers to remove\n","'''\n","\n","significant_features = df_encoded.corr()['TARGET'].abs().sort_values(ascending=False).head(10).index.tolist()\n","significant_features.remove('TARGET')\n","outliers_default, normal_default, total_outliers, pct_outliers = [], [], [], []\n","counter = 0\n","df_outliers =pd.DataFrame()\n","\n","print(f\"Top 9 features to analyze: {significant_features}\\n\")\n","\n","# IQR(Turkey Fence) method for outlier detection(np.percentile(data[column], 0.25))\n","def detect_outliers_iqr(data, column):\n","    Q1 = data[column].quantile(0.25) \n","    Q3 = data[column].quantile(0.75)\n","    IQR = Q3 - Q1\n","    return (data[column] < Q1 - 1.5*IQR) | (data[column] > Q3 + 1.5*IQR) #Both has be False for it not be outlier\n","\n","#Output how much outliers are in each Feature using Turkey Method\n","for feature in significant_features:\n","    outliers = detect_outliers_iqr(df_encoded, feature)\n","    total_outliers.append(outliers.sum())\n","    pct_outliers.append(100 * total_outliers[counter] / df_encoded.shape[0])\n","\n","    #How significant do outliers affect our mean/average of the TARGET\n","    if(math.isnan(df_encoded[outliers]['TARGET'].mean())):\n","        outliers_default.append(df_encoded[~outliers]['TARGET'].mean())\n","    else:\n","        outliers_default.append(df_encoded[outliers]['TARGET'].mean())\n","    normal_default.append(df_encoded[~outliers]['TARGET'].mean())\n","\n","    # Create outlier flag -- Converts Boolean Values to interger values\n","    df_encoded[f'{feature}_OUTLIER'] = outliers.astype(int)\n","    counter += 1\n","\n","df_outliers = pd.DataFrame({\"Features\": [ i for i in significant_features], \"Total Outliers\": total_outliers, \"Percentage Outliers\": pct_outliers,\n","                             \"Outlier Default Rate\": outliers_default, \"Normal Default Rate without Outliers\": normal_default\n","                            })\n","print(tabulate(df_outliers, headers=\"keys\"))\n","\n","\n","# Visualize with box plots\n","fig, axes = plt.subplots(2, 5, figsize=(15, 8))\n","axes = axes.flatten()\n","\n","for idx, feature in enumerate(significant_features,0):\n","    df_encoded.boxplot(column=feature, by='TARGET', ax=axes[idx])\n","    axes[idx].set_title(feature)\n","\n","plt.suptitle('Outliers by Target Variable', fontsize=14)\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"markdown","id":"18ee03a8","metadata":{},"source":["What was learnt through this step was that we found the approximate outliers for each of the nine most significant features to the model. Now we can replace in the dataframe where if its an outlier in the dataset it can be safely removed"]},{"cell_type":"code","execution_count":null,"id":"9d45f504","metadata":{"vscode":{"languageId":"shellscript"}},"outputs":[],"source":["print(f\"Before Total Observation with Duplicates of 9 Most Important Features of DF: {df_encoded.shape[0]}\")\n","outliers_col = [col for col in df_encoded.columns if 'OUTLIER' in col]\n","\n","#Filter rows where all outlier flags are 0[Dont want to remove just individual]\n","df_encoded = df_encoded[(df_encoded[outliers_col] == 0).all(axis=1)]\n","print(f\"After Observation without Duplicates of 9 Most Important Features of DF: {df_encoded.shape[0]}\")\n"]},{"cell_type":"code","execution_count":null,"id":"94a679e8-59b5-4ff4-82ab-ae2b802ed89e","metadata":{"id":"94a679e8-59b5-4ff4-82ab-ae2b802ed89e","outputId":"73dbdaa1-33fd-45fc-e059-1d494c3fff1f"},"outputs":[],"source":["#Income, Credit, and Annuity Distributions\n","fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n","for ax, col in zip(axes, ['AMT_INCOME_TOTAL', 'AMT_CREDIT', 'AMT_ANNUITY']):\n","    sns.histplot(data=df_encoded, x=np.log1p(df_train[col]), bins=50, kde=True, hue='TARGET', ax=ax)\n","    ax.set_title(f'Log Distribution of {col}')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"80f93168-a70d-4917-b917-fbf2a6a648a9","metadata":{"id":"80f93168-a70d-4917-b917-fbf2a6a648a9","outputId":"1b0f9141-1f8c-43f6-b7a8-0b219c380b29"},"outputs":[],"source":["#Age & Employment Duration (Convert days to years) \n","df_encoded['YEARS_BIRTH'] = -df_encoded['DAYS_BIRTH'] / 365\n","df_encoded['YEARS_EMPLOYED'] = df_encoded['DAYS_EMPLOYED'].apply(lambda x: np.nan if x > 0 else -x/365)\n","\n","fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n","sns.histplot(data=df_encoded, x='YEARS_BIRTH', hue='TARGET', bins=40, kde=True, ax=axes[0])\n","sns.histplot(data=df_encoded, x='YEARS_EMPLOYED', hue='TARGET', bins=40, kde=True, ax=axes[1])\n","axes[0].set_title('Age Distribution by Default Status')\n","axes[1].set_title('Employment Duration by Default Status')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"030428ea-e976-45a9-9403-c68457994d0e","metadata":{"id":"030428ea-e976-45a9-9403-c68457994d0e","outputId":"db3b4d66-e46e-4390-a548-469a3968d82d"},"outputs":[],"source":["#Categorical Features vs Default\n","categorical_features = ['NAME_INCOME_TYPE', 'OCCUPATION_TYPE', 'FLAG_OWN_REALTY', 'FLAG_OWN_CAR']\n","\n","for col in categorical_features:\n","    plt.figure(figsize=(8, 4))\n","    sns.countplot(data=df_train, x=col, hue='TARGET', order=df_train[col].value_counts().index)\n","    plt.xticks(rotation=45)\n","    plt.title(f'{col} vs Default Status')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"id":"ed2303a5-d86a-4525-b7ce-a805f28b6802","metadata":{"id":"ed2303a5-d86a-4525-b7ce-a805f28b6802","outputId":"6dea9b83-9788-46d5-e662-ddb585f55023"},"outputs":[],"source":["# Credit Bureau & Social Circle Features\n","cols = [\n","    'AMT_REQ_CREDIT_BUREAU_MON',\n","    'OBS_30_CNT_SOCIAL_CIRCLE',\n","    'DEF_30_CNT_SOCIAL_CIRCLE'\n","]\n","\n","fig, axes = plt.subplots(3, figsize=(14, 14))\n","for ax, col in zip(axes.flatten(), cols):\n","    sns.boxplot(data=df_train, x='TARGET', y=col, ax=ax)\n","    ax.set_title(f'{col} by Default Status')\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"id":"95a30afe","metadata":{},"outputs":[],"source":["categorical = [\"SK_ID_CURR\",\n","\"TARGET\",\n","\"NAME_CONTRACT_TYPE\",\n","\"CODE_GENDER\",\n","\"FLAG_OWN_CAR\",\n","\"FLAG_OWN_REALTY\",\n","\"CNT_CHILDREN\",\n","\"NAME_TYPE_SUITE\",\n","\"NAME_INCOME_TYPE\",\n","\"NAME_EDUCATION_TYPE\",\n","\"NAME_FAMILY_STATUS\",\n","\"NAME_HOUSING_TYPE\",\n","\"FLAG_MOBIL\",\n","\"FLAG_EMP_PHONE\",\n","\"FLAG_HOME_PHONE\",\n","\"FLAG_CONT_MOBILE\",\n","\"FLAG_PHONE\",\n","\"FLAG_EMAIL\",\n","\"OCCUPATION_TYPE\",\n","\"CNT_FAMILY_MEMBER\",\n","\"REGION_RATING_CLIENT\",\n","\"REGION_RATING_CLIENT_W_CITY\",\n","\"WEEKDAY_APPR_PROCESS_START\",\n","\"HOUR_APPR_PROCESS_START\",\n","\"REG_REGION_NOT_LIVE_REGION\",\n","\"REG_REGION_NOT_WORK_REGION\",\n","\"LIVE_REGION_NOT_WORK_REGION\",\n","\"REG_CITY_NOT_LIVE_CITY\",\n","\"REG_CITY_NOT_WORK_CITY\",\n","\"ORGANIZATION_TYPE\",\n","\"FONDKAPREMONT_MODE\",\n","\"HOUSETYPE_MODE\",\n","\"WALLSMATERIAL_MODE\",\n","\"EMERGENCYSTATE_MODE\",\n","\"DEF_30_CNT_SOCIAL_CIRCLE\",\n","\"DEF_60_CNT_SOCIAL_CIRCLE\",\n","\"DAYS_LAST_PHONE_CHANGE\",\n","\"FLAG_DOCUMENT_2\",\n","\"FLAG_DOCUMENT_3\",\n","\"FLAG_DOCUMENT_4\",\n","\"FLAG_DOCUMENT_5\",\n","\"FLAG_DOCUMENT_6\",\n","\"FLAG_DOCUMENT_7\",\n","\"FLAG_DOCUMENT_8\",\n","\"FLAG_DOCUMENT_9\",\n","\"FLAG_DOCUMENT_10\",\n","\"FLAG_DOCUMENT_11\",\n","\"FLAG_DOCUMENT_12\",\n","\"FLAG_DOCUMENT_13\",\n","\"FLAG_DOCUMENT_14\",\n","\"FLAG_DOCUMENT_15\",\n","\"FLAG_DOCUMENT_16\",\n","\"FLAG_DOCUMENT_17\",\n","\"FLAG_DOCUMENT_18\",\n","\"FLAG_DOCUMENT_19\",\n","\"FLAG_DOCUMENT_20\",\n","\"FLAG_DOCUMENT_21\",\n","\"AMT_REQ_CREDIT_BUREAU_HOUR\",\n","\"AMT_REQ_CREDIT_BUREAU_DAY\",\n","\"AMT_REQ_CREDIT_BUREAU_WEEK\",\n","\"AMT_REQ_CREDIT_BUREAU_MON\",\n","\"AMT_REQ_CREDIT_BUREAU_QRT\",\n","\"AMT_REQ_CREDIT_BUREAU_YEAR\",\n","\"STATUS\"\n","]\n","\n","continuous_numerical = [\n","\"AMT_INCOME_TOTAL\",\n","\"AMT_CREDIT\",\n","\"AMT_ANNUITY\",\n","\"AMT_GOODS_PRICE\",\n","\"REGION_POPULATION_RELATIVE\",\n","\"EXT_SOURCE_1\",\n","\"EXT_SOURCE_2\",\n","\"EXT_SOURCE_3\",\n","\"APARTMENTS_AVG\",\n","\"BASEMENTAREA_AVG\",\n","\"YEARS_BEGINEXPLUATATION_AVG\",\n","\"YEARS_BUILD_AVG\",\n","\"COMMONAREA_AVG\",\n","\"ELEVATORS_AVG\",\n","\"ENTRANCES_AVG\",\n","\"FLOORSMAX_AVG\",\n","\"FLOORSMIN_AVG\",\n","\"LANDAREA_AVG\",\n","\"LIVINGAPARTMENTS_AVG\",\n","\"LIVINGAREA_AVG\",\n","\"NONLIVINGAPARTMENTS_AVG\",\n","\"NONLIVINGAREA_AVG\",\n","\"APARTMENTS_MODE\",\n","\"BASEMENTAREA_MODE\",\n","\"YEARS_BEGINEXPLUATATION_MODE\",\n","\"YEARS_BUILD_MODE\",\n","\"COMMONAREA_MODE\",\n","\"ELEVATORS_MODE\",\n","\"ENTRANCES_MODE\",\n","\"FLOORSMAX_MODE\",\n","\"FLOORSMIN_MODE\",\n","\"LANDAREA_MODE\",\n","\"LIVINGAPARTMENTS_MODE\",\n","\"LIVINGAREA_MODE\",\n","\"NONLIVINGAPARTMENTS_MODE\",\n","\"NONLIVINGAREA_MODE\",\n","\"APARTMENTS_MEDI\",\n","\"BASEMENTAREA_MEDI\",\n","\"YEARS_BEGINEXPLUATATION_MEDI\",\n","\"YEARS_BUILD_MEDI\",\n","\"COMMONAREA_MEDI\",\n","\"ELEVATORS_MEDI\",\n","\"ENTRANCES_MEDI\",\n","\"FLOORSMAX_MEDI\",\n","\"FLOORSMIN_MEDI\",\n","\"LANDAREA_MEDI\",\n","\"LIVINGAPARTMENTS_MEDI\",\n","\"LIVINGAREA_MEDI\",\n","\"NONLIVINGAPARTMENTS_MEDI\",\n","\"NONLIVINGAREA_MEDI\",\n","\"TOTALAREA_MODE\"\n","]\n","\n","discrete_numerical = [\n","\"DAYS_BIRTH\",\n","\"DAYS_EMPLOYED\",\n","\"DAYS_REGISTRATION\",\n","\"DAYS_ID_PUBLISH\",\n","\"OWN_CAR_AGE\",\n","\"OBS_30_CNT_SOCIAL_CIRCLE\",\n","\"OBS_60_CNT_SOCIAL_CIRCLE\"\n","]\n","df = pd.DataFrame(columns=[\"CATEGORICAL_FEATURE\", \"DISCRETE\", \"CONTINUOUS\"])\n","df[\"CATEGORICAL_FEATURE\"] = 0\n","df[\"DISCRETE\"] = 0\n","df[\"CONTINUOUS\"] = 0\n","\n","for x in df_encoded.columns:\n","    if x in categorical:\n","        df[\"CATEGORICAL_FEATURE\"] =  1\n","    if x in discrete_numerical:\n","        df[\"DISCRETE\"] = 1\n","    if x in continuous_numerical:\n","        df[\"CONTINUOUS\"] = 1"]},{"cell_type":"code","execution_count":null,"id":"56b4c3ae","metadata":{},"outputs":[],"source":["#I may want to do some transformation of data (We should do something about it)\n","df_copy = df_train.copy()\n","# sns.pairplot(df_train[[\"DAYS_BIRTH\", \"TARGET\"]])\n","df_copy[\"DAYS_BIRTH\"] = np.log( -1 * df_train[\"DAYS_BIRTH\"])\n","# sns.histplot(df_train[\"DAYS_BIRTH\"])\n","print(df_copy[\"DAYS_BIRTH\"].head(10))\n","sns.histplot(df_copy[\"DAYS_BIRTH\"]) #Transformation was not good"]},{"cell_type":"markdown","id":"1895449b","metadata":{},"source":["## Data Preprocessing\n","\n","1) Handle missing data using Deletion or Imputation techniques\n","2) Encode Categorical Variables\n","3) Scale and Normalize Features\n","4) Remove or Cap Outliers\n","5) Balance Dataset prevent Class Imbalance using SMOTE\n","6) Feature Selection\n","7) Split into Train and Test Sets --> Done for us\n"]},{"cell_type":"code","execution_count":null,"id":"1111374a","metadata":{},"outputs":[],"source":["%pip install scipy\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from scipy import stats\n","# Generate random data following a normal distribution\n","mean = 0  # Mean of the distribution\n","std_dev = 1  # Standard deviation\n","num_samples = 1000  # Number of data points\n","\n","# Generate data points\n","data = np.random.normal(loc=mean, scale=std_dev, size=num_samples)\n","\n","# Plot the data\n","plt.figure(figsize=(10,6))\n","sns.histplot(data, kde=True, bins=30, color='skyblue', stat='density')\n","\n","# Add title and labels\n","plt.title('Normal Distribution with Mean = 0 and Std Dev = 1', fontsize=16)\n","plt.xlabel('Value', fontsize=12)\n","plt.ylabel('Density', fontsize=12)\n","\n","# Display the plot\n","plt.show()\n","\n","# Descriptive statistics\n","print(f\"Mean: {np.mean(data):.2f}\")\n","print(f\"Standard Deviation: {np.std(data):.2f}\")\n","print(f\"Skewness: {stats.skew(data):.2f}\")\n","print(f\"Kurtosis: {stats.kurtosis(data):.2f}\")\n"]},{"cell_type":"code","execution_count":null,"id":"f15e6ef5","metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.impute import SimpleImputer\n","\n","df_learn = df_train[[\"AMT_GOODS_PRICE\", \"OWN_CAR_AGE\", \"AMT_ANNUITY\"]]\n","print(df_learn.columns)\n","print(df_learn.isnull().sum())\n","imput_mode = SimpleImputer(strategy=\"most_frequent\")\n","values  = imput_mode.fit_transform(df_learn) #New valuesof columns\n","df_learn.loc[:,[\"AMT_GOODS_PRICE\", \"OWN_CAR_AGE\", \"AMT_ANNUITY\"]] = values\n","print(df_learn.isnull().sum())\n","print(df_learn.head(10))\n"]},{"cell_type":"markdown","id":"80184b87","metadata":{},"source":["##### Autoglon\n","\n","- Autoglon has suggested many good models to test for and of the models the three best models it says are LightGBMXT, LightGBM, RandomForestGini which are all Tree Based Modles[Capture Non-linear relationships which means we probably have a lot]\n","\n","- It also provides us information we can use when doing feature engineering or model tuning on which variables to keep:\n","    - 'FLAG_DOCUMENT_10', 'FLAG_DOCUMENT_12', 'FLAG_DOCUMENT_17', 'FLAG_DOCUMENT_19' --> Features can also be unused if they carry very little information, such as being categorical but having almost entirely unique values or being duplicates of other features.\n","\n","    - ['FLAG_OWN_CAR', 'FLAG_MOBIL', 'FLAG_DOCUMENT_2', 'FLAG_DOCUMENT_4'] --> These features carry no predictive signal and should be manually investigated.\n","\t\tThis is typically a feature which has the same value for all rows.\n","\t\tThese features do not need to be present at inference time."]},{"cell_type":"markdown","id":"83459f8d","metadata":{},"source":["### Creating KFold Cross Validations"]},{"cell_type":"markdown","id":"1498a62b","metadata":{},"source":["#### Notes\n","- From the Baseline Models we notice that althought we might have a super high accuracy score our ROC Curve, Precision Score, F1 Score, Recall score all blowout and are really low. From this we also know that because we have a class imbalance of 90% non-default vs 10% default our accuracy score is really bias as its only telling us what the chances of it predicting that the observation is non-default, it leans more towards the majority class. As a result we need imbalancing methods like SMOTE or Oversampling or a Hybrid Method to make sure that our minority class also shows up more often. \n","\n","- As we compare the baseline models as we wanna see how well it does with just raw data we notice that all of them because of class imbalance score poorly in every other metric other than accuracy. However Log Reg seems to do the best in the case of precision, recall, f1 score compared to the other models. As a result, we will use Log Reg as one of model to train on SMOTE. On the other hand, Random Forest Classifier also does fine and we should also test SMOTE on it"]},{"cell_type":"markdown","id":"d95d9aac","metadata":{},"source":["### Training and Creating our Log Reg Model and RF Model [Still not Hyperparm Tuning]"]},{"cell_type":"markdown","id":"50a97b15","metadata":{},"source":["### Things to Consider\n","    - Feature Engineering and Feature Selection\n","    - Interactions\n","    - More Diagrams and Visualization\n","    - Clean up Variable Table"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":5}
